---
title: "Lab 6"
author: "Your Name Here"
output: pdf_document
date: "11:59PM March 30"
---


#Logistic Regression

Let's consider the Pima Indians Diabetes dataset from 1988:

```{r}
?MASS::Pima.tr2
skimr::skim(MASS::Pima.tr2)
y = ifelse(MASS::Pima.tr2$type == "Yes", 1, 0)
X = cbind(1, MASS::Pima.tr2[, 1 : 7])
```

Note the missing data. We will learn about how to handle missing data towards the end of the course. For now, replace, the missing data in the design matrix X with the average of the feature x_dot,j. You can check that this worked with the table commands at the end of the chunk:

```{r}
#TO-DO

table(X$bp, useNA = "always")
table(X$skin, useNA = "always")
table(X$bmi, useNA = "always")
```

Now let's fit a log-odds linear model of y=1 (type is "diabetic") on just the `glu` variable. Use `optim` to fit the model.

```{r}
#TO-DO
```

Masters students: write a `fit_logistic_regression` function which takes in X, y and returns b which uses the optimization routine.

```{r}
fit_logistic_regression = function(X, y){
  b = #TO-DO
  b
}
```

Run a logistic regression of y=1 (type is "diabetic") on just the `glu` variable using R's built-in function and report b_0, b_1.

```{r}
#TO-DO
```

Comment on how close the results from R's built-in function was and your optimization call.

#TO-DO

Interpret the value of b_1 from R's built-in function.

#TO-DO

Interpret the value of b_0 from R's built-in function.

#TO-DO

Plot the probability of y=1 from the minimum value of `glu` to the maximum value of `glu`.

```{r}
#TO-DO
```

Run a logistic regression of y=1 (type is "diabetic") on all variables using R's built-in function and report the b vector.

```{r}
coef(glm(y ~ X[, "glu"], family = "binomial"))
```

Predict the probability of diabetes for someone with a blood sugar of 150.

```{r}
#TO-DO
```

For 100 people with blood sugar of 150, what is the probability more than 75 of them have diabetes? (You may need to review 241 to do this problem).

```{r}
#TO-DO
```

Plot the in-sample log-odds predictions (y-axis) versus the real response values (x-axis).

```{r}
#TO-DO
```

Plot the in-sample probability predictions (y-axis) versus the real response values (x-axis).

```{r}
#TO-DO
```

Comment on how well you think the logistic regression performed in-sample.

#TO-DO

Calculate the in-sample Brier score.

```{r}
#TO-DO
```

Calculate the in-sample log-scoring rule.

```{r}
#TO-DO
```


Run a probit regression of y=1 (type is "diabetic") on all variables using R's built-in function and report the b vector.


```{r}
#TO-DO
```

Does the weight estimates here in the probit fit have different signs than the weight estimates in the logistic fit? What does that mean?

#TO-DO

Plot the in-sample probability predictions (y-axis) versus the real response values (x-axis).

```{r}
#TO-DO
```

Calculate the in-sample Brier score.

```{r}
#TO-DO
```

Calculate the in-sample log-scoring rule.

```{r}
#TO-DO
```

Which model did better in-sample?

#TO-DO

Compare both models oos using the Brier score and a test set with 1/3 of the data.

```{r}
#TO-DO
```

Which model did better oos?

#TO-DO




#Polynomial Regression and Interaction Regression

We will work with the diamonds dataset again. Here we load up the dataset and convert all factors to nominal type:

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
diamonds = ggplot2::diamonds
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal
skimr::skim(diamonds)
```

Given the information above, what are the number of columns in the raw X matrix?

#TO-DO

Verify this using code:

```{r}
#TO-DO
```

Would it make sense to use polynomial expansions for the variables cut, color and clarity? Why or why not?

#TO-DO

Would it make sense to use log transformations for the variables cut, color and clarity? Why or why not?

#TO-DO

In order to ensure there is no time trend in the data, randomize the order of the diamond observations in D:.

```{r}
#TO-DO
```

Let's also concentrate only on diamonds with <= 2 carats to avoid the issue we saw with the maximum. So subset the dataset. Create a variable n equal to the number of remaining rows as this will be useful for later. Then plot it.

```{r}
#TO-DO
```

Create a linear model of price ~ carat and gauge its in-sample performance using s_e.

```{r}
#TO-DO
```

Create a model of price ~ clarity and gauge its in-sample performance

```{r}
#TO-DO
```

Why is the model price ~ carat substantially more accurate than price ~ clarity?

#TO-DO

Create a new transformed feature ln_carat and plot it vs price.

```{r}
#TO-DO
```

Would price ~ ln_carat be a better fitting model than price ~ carat? Why or why not?

#TO-DO

Verify this by comparing R^2 and RMSE of the two models:

```{r}
#TO-DO
```

Create a new transformed feature ln_price and plot its estimated density:


```{r}
#TO-DO
ggplot(diamonds) + geom_histogram(aes(x = ln_price), binwidth = 0.01)
```


Now plot it vs carat.

```{r}
ggplot(diamonds, aes(x = carat, y = ln_price)) + 
  geom_point()
```

Would ln_price ~ carat be a better fitting model than price ~ carat? Why or why not?

#TO-DO

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
#TO-DO
```

We just compared in-sample statistics to draw a conclusion on which model has better performance. But in-sample statistics can lie! Why is what we did valid?

#TO-DO

Plot ln_price vs ln_carat.

```{r}
#TO-DO
```

Would ln_price ~ ln_carat be the best fitting model than the previous three we considered? Why or why not?

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
#TO-DO
```

Compute b, the OLS slope coefficients for this new model of ln_price ~ ln_carat.

```{r}
#Model A
#TO-DO
```

Interpret b_1, the estimated slope of ln_carat.

#TO-DO

Interpret b_0, the estimated intercept.

#TO-DO

Create other features ln_x, ln_y, ln_z, ln_depth, ln_table.

```{r}
#TO-DO
```
