---
title: "Lab 8"
author: "Your Name Here"
output: pdf_document
---


#Visualization with the package ggplot2

I highly recommend using the [ggplot cheat sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) as a reference resource. You will see questions that say "Create the best-looking plot". Among other things you may choose to do, remember to label the axes using real English, provide a title and subtitle. You may want to pick a theme and color scheme that you like and keep that constant throughout this lab. The default is fine if you are running short of time.

Load up the `GSSvocab` dataset in package `carData` as `X` and drop all observations with missing measurements. This will be a very hard visualization exercise since there is not a good model for vocab.

```{r}
#TO-DO
```

Briefly summarize the documentation on this dataset. What is the data type of each variable? What do you think is the response variable the collectors of this data had in mind?

#TO-DO

Create two different plots and identify the best-looking plot you can to examine the `age` variable. Save the best looking plot as an appropriately-named PDF.

```{r}
#TO-DO
```

Create two different plots and identify the best looking plot you can to examine the `vocab` variable. Save the best looking plot as an appropriately-named PDF.

```{r}
#TO-DO
```

Create the best-looking plot you can to examine the `ageGroup` variable by `gender`. Does there appear to be an association? There are many ways to do this.

```{r}
#TO-DO
```

Create the best-looking plot you can to examine the `vocab` variable by `age`. Does there appear to be an association?

```{r}
#TO-DO
```

Add an estimate of $f(x)$ using the smoothing geometry to the previous plot. Does there appear to be an association now?

```{r}
#TO-DO
```

Using the plot from the previous question, create the best looking plot overloading with variable `gender`. Does there appear to be an interaction of `gender` and `age`?

```{r}
#TO-DO
```


Using the plot from the previous question, create the best looking plot overloading with variable `nativeBorn`. Does there appear to be an interaction of `nativeBorn` and `age`?

```{r}
#TO-DO
```

Create two different plots and identify the best-looking plot you can to examine the `vocab` variable by `educGroup`. Does there appear to be an association?

```{r}
#TO-DO
```

Using the best-looking plot from the previous question, create the best looking overloading with variable `gender`. Does there appear to be an interaction of `gender` and `educGroup`?

```{r}
#TO-DO
```

Using facets, examine the relationship between `vocab` and `ageGroup`. You can drop year level `(Other)`. Are we getting dumber?

```{r}
#TO-DO
```

#Rcpp and optimizing R

Write a function `dot_product_R` in R that takes in two vectors `v1` and `v2` and returns their dot product.

```{r}
#TO-DO
```

Write a function `dot_product_cpp` in C++ and make sure it compiles.

```{r}
cppFunction('
  double dot_product_cpp(NumericVector v1, NumericVector v2) {
    #TO-DO
  }
')
```

Create two vectors of standard normal realizations with length `n=1e6` and test the different in speed.

```{r}
n = 1e6
v1 = rnorm(n)
v2 = rnorm(n)

pacman::p_load(microbenchmark)
microbenchmark(
  dot_product_R(v1, v2), 
  dot_product_cpp(v1, v2),
  times = 10
)
```

Implement the Gram Schmidt routine as a C++ function `gram_schmidt_cpp`.

```{r}
#TO-DO
```

Here is the implementation in R for reference taken from lab 5:

```{r}
gram_schmidt_R = function(X){
  #first create orthogonal matrix
  V = matrix(NA, nrow = nrow(X), ncol = ncol(X))
  V[, 1] = X[, 1]
  
  for (j in 2 : ncol(X)){
    V[, j] = X[, j]
    
    for (k in 1 : (j-1)){
      v_k = V[, k, drop = FALSE]
      V[, j] = V[, j, drop = FALSE] - (t(t(v_k)) %*% t(v_k) / sum(v_k^2)) %*% t(t(X[, j])) #i.e. the orthogonal projection of X[, j] onto v_k
    }
  }
  
  Q = matrix(NA, nrow = nrow(X), ncol = ncol(X))
  for (j in 1 : ncol(X)){
    Q[, j] = V[, j] / sqrt(sum(V[, j]^2))
  }
  Q
}
```

Now let's see how much faster C++ is by running it on the boston housing data design matrix
```{r}
X = model.matrix(medv ~ ., MASS::Boston)

microbenchmark(
  gram_schmidt_R(X),
  gram_schmidt_cpp(X),
  times = 10
)
```

Create a variable `n` to be 10 and a vaiable `Nvec` to be 100 initially. Create a random vector via `rnorm` `Nvec` times and load it into a `Nvec` x `n` dimensional matrix.

```{r}
#TO-DO
```

Write a function `all_angles` that measures the angle between each of the pairs of vectors. You should measure the vector on a scale of 0 to 180 degrees with negative angles coerced to be positive.

```{r}
#TO-DO
```

Plot the density of these angles.

```{r}
#TO-DO
```

Write an Rcpp function `all_angles_cpp` that does the same thing. Use an IDE if you want, but write it below in-line.

```{r}
#TO-DO
```

Test the time difference between these functions for `n = 1000` and `Nvec = 100, 500, 1000, 5000` using the package `microbenchmark`.  Store the results in a matrix with rows representing `Nvec` and two columns for base R and Rcpp.

```{r}
Nvecs = c(100, 500, 1000, 5000)

results_for_time = data.frame(
  Nvec = Nvecs,
  time_for_base_R = numeric(),
  time_for_cpp = numeric()
)
for (i in 1 : length(Nvecs)){
  X = matrix(rnorm(n * Nvecs[i]), nrow = Nvec)
  results_for_time$time_for_base_R[i] = all_angles(X)
  results_for_time$time_for_cpp[i] = all_angles_cpp(X)
}

ggplot(results_for_time) + 
  geom_line(aes(x = Nvec, y = time_for_base_R), col = "red") +
  geom_line(aes(x = Nvec, y = time_for_cpp), col = "blue")
```

Plot the divergence of performance (in log seconds) over n using a line geometry. Use two different colors for the R and CPP functions. Make sure there's a color legend on your plot. We wil see later how to create "long" matrices that make such plots easier.

```{r}
#TO-DO
```

Let `Nvec = 10000` and vary `n` to be 10, 100, 1000. Plot the density of angles for all three values of `n` on one plot using color to signify `n`. Make sure you have a color legend. This is not easy.

```{r}
#TO-DO
```

Write an R function `nth_fibonnaci` that finds the nth Fibonnaci number via recursion but allows you to specify the starting number. For instance, if the sequence started at 1, you get the familiar 1, 1, 2, 3, 5, etc. But if it started at 0.01, you would get 0.01, 0.01, 0.02, 0.03, 0.05, etc.

```{r}
#TO-DO
```

Write an Rcpp function `nth_fibonnaci_cpp` that does the same thing. Use an IDE if you want, but write it below in-line.

```{r}
#TO-DO
```

Time the difference in these functions for n = 100, 200, ...., 1500 while starting the sequence at the smallest possible floating point value in R. Store the results in a matrix.

```{r}
#TO-DO
```

Plot the divergence of performance (in log seconds) over n using a line geometry. Use two different colors for the R and CPP functions. Make sure there's a color legend on your plot.

```{r}
#TO-DO
```


# Data Wrangling / Munging / Carpentry

Throughout this assignment you should use `dplyr` with `magrittr` piping. I'll be writing the data.table code for you after you're done so you can see it as it may be useful for your future.

```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

Load the `storms` dataset from the `dplyr` package and read about it using `?storms` and summarize its data via `skimr:skim`. 

```{r}
storms = dplyr::storms
?storms
skimr::skim(storms)
head(storms)
```

To make the modeling exercise easier, let's eliminate rows that have missingness in `tropicalstorm_force_diameter` or `hurricane_force_diameter`.

```{r}
storms = #TO-DO
skimr::skim(storms)
```

Which column(s) should be converted to type factor? Do the conversion:

```{r}
#TO-DO
```

Reorder the columns so name is first, status is second, category is third and the rest are the same.

```{r}
#TO-DO
```

Find a subset of the data of storms only in the 1970's.

```{r}
#TO-DO
```

Find a subset of the data of storm observations only with category 4 and above and wind speed 100MPH and above.

```{r}
#TO-DO
```

Create a new feature `wind_speed_per_unit_pressure`.

```{r}
#TO-DO
```

Create a new feature: `average_diameter` which averages the two diameter metrics. If one is missing, then use the value of the one that is present. If both are missing, leave missing.

```{r}
#TO-DO
```


For each storm, summarize the maximum wind speed. "Summarize" means create a new dataframe with only the summary metrics you care about.

```{r}
#TO-DO
```

Order your dataset by maximum wind speed storm but within the rows of storm show the observations in time order from early to late.

```{r}
#TO-DO
```

Find the strongest storm by wind speed per year.

```{r}
distinct(storms[, max_wind_by_year := max(wind), by = year][wind == max_wind_by_year, .(year, name, wind)])[, .(year, name)]

storms %>%
  group_by(year) %>%
  filter(wind == max(wind)) %>%
  select(year, name, wind) %>%
  distinct %>%
  select(year, name)
```

For each named storm, find its maximum category, wind speed, pressure and diameters. Do not allow the max to be NA (unless all the measurements for that storm were NA).

```{r}
#TO-DO
```


For each year in the dataset, tally the number of storms. "Tally" is a fancy word for "count the number of". Plot the number of storms by year. Any pattern?

```{r}
data(storms)
storms %>% 
  group_by(year) %>% 
  summarize(num_storms = n_distinct(name))

```

For each year in the dataset, tally the storms by category.

```{r}
#TO-DO
```

For each year in the dataset, find the maximum wind speed per status level.

```{r}
#TO-DO
```

For each storm, summarize its average location in latitude / longitude coordinates.

```{r}
#TO-DO
```

For each storm, summarize its duration in number of hours (to the nearest 6hr increment).

```{r}
#TO-DO
```

For storm in a category, create a variable `storm_number` that enumerates the storms 1, 2, ... (in date order).

```{r}
#TO-DO
```

Convert year, month, day, hour into the variable `timestamp` using the `lubridate` package. Although the new package `clock` just came out, `lubridate` still seems to be standard. Next year I'll probably switch the class to be using `clock`.

```{r}
#TO-DO
```

Using the `lubridate` package, create new variables `day_of_week` which is a factor with levels "Sunday", "Monday", ... "Saturday" and `week_of_year` which is integer 1, 2, ..., 52.

```{r}
#TO-DO
```

For each storm, summarize the day in which is started in the following format "Friday, June 27, 1975".

```{r}
#TO-DO
```

Create a new factor variable `decile_windspeed` by binning wind speed into 10 bins.

```{r}
#TO-DO
```

Create a new data frame `serious_storms` which are category 3 and above hurricanes.

```{r}
#TO-DO
```

In `serious_storms`, merge the variables lat and long together into `lat_long` with values `lat / long` as a string.

```{r}
#TO-DO
```

Let's return now to the original storms data frame. For each category, find the average wind speed, pressure and diameters (do not count the NA's in your averaging).

```{r}
#TO-DO
```

For each named storm, find its maximum category, wind speed, pressure and diameters (do not allow the max to be NA) and the number of readings (i.e. observations).

```{r}
#TO-DO
```

Calculate the distance from each storm observation to Miami in a new variable `distance_to_miami`. This is very challenging. You will need a function that computes distances from two sets of latitude / longitude coordinates. 

```{r}
MIAMI_LAT_LONG_COORDS = c(25.7617, -80.1918)
#TO-DO
```

For each storm observation, use the function from the previous question to calculate the distance it moved since the previous observation.

```{r}
#TO-DO
```

For each storm, find the total distance it moved over its observations and its total displacement. "Distance" is a scalar quantity that refers to "how much ground an object has covered" during its motion. "Displacement" is a vector quantity that refers to "how far out of place an object is"; it is the object's overall change in position.

```{r}
#TO-DO
```

For each storm observation, calculate the average speed the storm moved in location.

```{r}
#TO-DO
```

For each storm, calculate its average ground speed (how fast its eye is moving which is different from windspeed around the eye).

```{r}
#TO-DO
```

Is there a relationship between average ground speed and maximum category attained? Use a dataframe summary (not a regression).

```{r}
#TO-DO
```

Now we want to transition to building real design matrices for prediction. This is more in tune with what happens in the real world. Large data dump and you convert it into $X$ and $y$ how you see fit.

Suppose we wish to predict the following: given the first three readings of a storm, can you predict its maximum wind speed? Identify the `y` and identify which features you need $x_1, ... x_p$ and build that matrix with `dplyr` functions. This is not easy, but it is what it's all about. Feel free to "featurize" as creatively as you would like. You aren't going to overfit if you only build a few features relative to the total 198 storms.

```{r}

```

Fit your model. Validate it. 
 
```{r}
#TO-DO
```

Assess your level of success at this endeavor.

#TO-DO


# More data munging with table joins


```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

We will be using the `storms` dataset from the `dplyr` package. Filter this dataset on all storms that have no missing measurements for the two diameter variables, "tropicalstorm_force_diameter" and "hurricane_force_diameter". Zeroes count as missing as well.

```{r}
#TO-DO
```

From this subset, create a data frame that only has storm name, observation period number for each storm (i.e., 1, 2, ..., T) and the "tropicalstorm_force_diameter" and "hurricane_force_diameter" metrics.

```{r}
#TO-DO
```

Create a data frame in long format with columns "diameter" for the measurement and "diameter_type" which will be categorical taking on the values "hu" or "ts".

```{r}
#TO-DO
```

Using this long-formatted data frame, use a line plot to illustrate both "tropicalstorm_force_diameter" and "hurricane_force_diameter" metrics by observation period for four random storms using a 2x2 faceting. The two diameters should appear in two different colors and there should be an appropriate legend.

```{r}
#TO-DO
```
