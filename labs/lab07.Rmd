---
title: "Lab 7"
author: "Your Name Here"
output: pdf_document
---

#Model Selection with Three Splits: Select from M models

We employ the diamonds dataset and specify M models nested from simple to more complex. We store the models as strings in a list (i.e. a hashset)

```{r}
?ggplot2::diamonds
model_formulas = c(
  "carat",
  "carat + cut",
  "carat + cut + color",
  "carat + cut + color + clarity",
  "carat + cut + color + clarity + x + y + z",
  "carat + cut + color + clarity + x + y + z + depth",
  "carat + cut + color + clarity + x + y + z + depth + table",
  "carat * (cut + color + clarity) + x + y + z + depth + table",
  "(carat + x + y + z) * (cut + color + clarity) + depth + table",
  "(carat + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity + poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table))"
)
model_formulas = paste0("price ~ ", model_formulas)
M = length(model_formulas)
```

In order to use the formulas with logs we need to eliminate rows with zeros in those measurements:

```{r}
diamonds_cleaned = ggplot2::diamonds
diamonds_cleaned = diamonds_cleaned[
  diamonds_cleaned$carat > 0 &
  diamonds_cleaned$x > 0 &
  diamonds_cleaned$y > 0 &
  diamonds_cleaned$z > 0 &
  diamonds_cleaned$depth > 0 &
  diamonds_cleaned$table > 0, #all columns
]
```

Split the data into train, select and test. Each set should have 1/3 of the total data.

```{r}
n = nrow(diamonds_cleaned)
set.seed(1)
train_idx = sample(1 : n, round(n / 3))
select_idx = sample(setdiff(1 : n, train_idx), round(n / 3))
test_idx = setdiff(1 : n, c(train_idx, select_idx))
diamonds_train =  diamonds_cleaned[train_idx, ]
diamonds_select = diamonds_cleaned[select_idx, ]
diamonds_test =   diamonds_cleaned[test_idx, ]
```

Find the oosRMSE on the select set for each model. Save the number of df in each model while you're doing this as we'll need it for later.

```{r}
#TO-DO
```

Plot the oosRMSE by model complexity (df in model)

```{r}
pacman::p_load(ggplot2)
#TO-DO
```

Select the best model by oosRMSE and find its oosRMSE on the test set.

```{r}
#TO-DO
```

Did we overfit the select set? Discuss why or why not.

#TO-DO

Create the final model object `g_final`.

```{r}
#TO-DO
```


#Model Selection with Three Splits: Hyperparameter selection

We will use an algorithm that I historically taught in 324W but now moved to 343 so I can teach it more deeply using the Bayesian topics from 341. The regression algorithm is called "ridge" and it involves solving for the slope vector via:

b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y

Note how if lambda = 0, this is the same algorithm as OLS. If lambda becomes very large then b_ridge is pushed towards all zeroes. So ridge is good at weighting only features that matter.

However, lambda is a hyperparameter >= 0 that needs to be selected.

We will work with the boston housing dataset except we will add 250 garbage features consisting of iid N(0,1) realizations. We will also standardize the columns so they're all xbar = 0 and s_x = 1. This is shown to be important in 343.

```{r}
rm(list = ls())
?MASS::Boston
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_garbage = 250
set.seed(1)
X = cbind(X, matrix(rnorm(n * p_garbage), nrow = n))
X = apply(X, 2, function(x_dot_j){
                  (x_dot_j - mean(x_dot_j)) / sd(x_dot_j)
                })
X[, 1] = 1 #we standardized the intercept column which became zeroes - make it an intercept again
dim(X)
```


Now we split it into 300 train, 100 select and 106 test. 

```{r}
set.seed(1)
train_idx = sample(1 : n, 300)
select_idx = sample(setdiff(1 : n, train_idx), 100)
test_idx = setdiff(1 : n, c(train_idx, select_idx))
#TO-DO
```

We now create a grid of M = 200 models indexed by lambda. The lowest lambda should be zero (which is OLS) and the highest lambda can be 100.

```{r}
M = 200
lambda_grid = seq(from = 0, to = 100, length.out = M)
```

Now find the oosRMSE on the select set on all models each with their own lambda value.

```{r}
#TO-DO
```

Plot the oosRMSE by the value of lambda.

```{r}
#TO-DO
```

Select the model with the best oosRMSE on the select set and find its oosRMSE on the test set.

```{r}
#TO-DO
```

Create the final model object `g_final`.

```{r}
#TO-DO
```


#Model Selection with Three Splits: Forward stepwise modeling

We will use the adult data

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
n = nrow(adult)
?adult
#let's remove "education" as its duplicative with education_num
adult$education = NULL
```


To implement forward stepwise, we need a "full model" that contains anything and everything we can possible want to use as transformed predictors. Let's first create log features of all the numeric features. Instead of pure log, use log(value + 1) to handle possible zeroes.

```{r}
skimr::skim(adult)
#this gives us the list of numeric features to create logs
adult$log_age = log(adult$age + 1)
adult$log_fnlwgt = log(adult$fnlwgt + 1)
adult$log_education_num = log(adult$education_num + 1)
adult$log_capital_gain = log(adult$capital_gain + 1)
adult$log_capital_loss = log(adult$capital_loss + 1)
adult$log_hours_per_week = log(adult$hours_per_week + 1)
```

Now let's create a model matrix Xfull that contains all first order interactions. How many degrees of freedom in this "full model"?

```{r}
#TO-DO
```

Now let's split it into train, select and test sets. Because this will be a glm, model-building (training) will be slow, so let's keep the training set small at 2,000. Since prediction is fast, we can divide the others evenly among select and test.

```{r}
y = ifelse(adult$income == ">50K", 1, 0)
#TO-DO
Xfull_train =  Xfull[train_idx, ]
Xfull_select = Xfull[select_ids, ]
Xfull_test =   Xfull[test_idx, ]
y_train =      y[train_idx]
y_select =     y[select_idx]
y_test =       y[test_idx]
```

Now let's use the code from class to run the forward stepwise modeling. As this is binary classification, let's use logistic regression and to measure model performance, let's use the Brier score. Compute the Brier score in-sample (on training set) and oos (on selection set) for every iteration of j, the number of features selected from the greedy selection procedure.

```{r}
#TO-DO
```

Plot the in-sample Brier score (in red) and oos Brier score (in blue) by the number of features used.

```{r}
#TO-DO
```

Select the model with the best oos Brier score on the select set and find its oos Brier score on the test set.

```{r}
#TO-DO
```

Create the final model object `g_final`.

```{r}
#TO-DO
```
