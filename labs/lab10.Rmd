---
title: "Lab 10"
author: "Your Name Here"
output: pdf_document
---

#YARF

For the next couple of labs, I want you to make some use of a package I wrote that offers convenient and flexible tree-building and random forest-building. Make sure you have RTools installed (if you're on windows) and then a JDK installed:

https://www.oracle.com/java/technologies/downloads/

Then try to install rJava

```{r}
options(java.parameters = "-Xmx8000m")
pacman::p_load(rJava)
.jinit()
```

If you have error, messages, try to google them. Everyone has trouble with rJava!

If that worked, please try to run the following which will install YARF from my github:

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
pacman::p_load(YARF)
```

Please try to fix the error messages (if they exist) as best as you can. I can help on slack.

# Regression Trees

You can use the `YARF` package if it works, otherwise, use the `randomForest` package (the canonical R package for this algorithm).

Let's take a look at a simulated sine curve. Below is the code for the data generating process:

```{r}
rm(list = ls())
n = 500
sigma = 0.3
x_min = 0
x_max = 10
x = runif(n, x_min, x_max)

f_x = function(x){sin(x)}
x = runif(n, x_min, x_max)
y = f_x(x) + rnorm(n, 0, sigma)
```

Provide the bias-variance decomposition of this DGP fit with the tree model fitted with the optimal N_0 from the previous lab. It is a lot of code, but it is in the practice lectures. If your three numbers don't add up within two significant digits, increase your resolution.

```{r}
#TO-DO
```

# Classification Trees

Let's get the letter recognition data from the `mlbench` package.

```{r}
rm(list = ls())
pacman::p_load(mlbench)
data(LetterRecognition, package = "mlbench")
n = nrow(LetterRecognition)
skimr::skim(LetterRecognition)
```

This dataset has 20,000 examples. Create a training-select-test split so that they each have 1,000 observations.

```{r}
set.seed(1)
train_idx = sample(1 : n, 1000)
select_idx = sample(setdiff(1 : n, train_idx), 1000)
test_idx = sample(setdiff(1 : n, c(train_idx, select_idx)), 1000)
letters_train = LetterRecognition[train_idx, ]
letters_select = LetterRecognition[select_idx, ]
letters_test = LetterRecognition[test_idx, ]
```

Find the optimal classification tree by using the model selection algorithm to optimize the nodesize hyperparameter. Use misclassification error as the performance metric.

```{r}
nodesizes = seq(1, 50, by = 1)
misclassification_errs = array(NA, length(nodesizes))
for (m in 1 : length(nodesizes)){
  tree_mod = YARFCART(letters_train[, -1], letters_train[, 1], nodesize = nodesizes[m])
  misclassification_errs[m] = mean(letters_select[, 1] != predict(tree_mod, letters_select[, -1]))
}
```

Plot the oos misclassification error by nodesize.

```{r}
ggplot(data.frame(nodesize = nodesizes, misclassification_error = misclassification_errs)) + 
  aes(x = nodesize, y = misclassification_error) +
  geom_point() + 
  geom_line()
```

Construct the optimal classification tree on the union of train set and select set. Then estimate generalization error on test set. Save `y_hat_test` as we'll need it later.

```{r}
tree_mod_opt = #TO-DO
```

Print out the top of the tree so we can have some level of interpretation to how the model g is predicting.

```{r}
illustrate_trees(tree_mod_opt, max_depth = 5, length_in_px_per_half_split = 30, open_file = TRUE)
```

Create a "confusion matrix". This means it shows every predicted level (which is a letter in our case) and every actual level. Here you'll see every type of error e.g. "P was predicted but the real letter is H", "M was predicted but the real letter is N" etc. This is really easy: one call to the `table` function is all you need.

```{r}
#TO-DO
```

Which errors are most prominent in this model?

#TO-DO

#Bagged Trees and Random Forest

Take a training sample of n = 2000 observations from the diamonds data.

```{r}
rm(list = ls())
pacman::p_load(tidyverse)
set.seed(1)
diamonds_train = ggplot2::diamonds %>% 
  sample_n(2000)
```


Using the diamonds data, find the oob s_e for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can create the bagged tree model via setting an argument within the RF constructor function. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
for (m in 1 : length(num_trees_values)){
  tree_mod = YARFBAG(diamonds_train[, -7], diamonds_train[, 7, drop = TRUE], num_trees = num_trees_values[m])
  oob_se_bagged_trees_mod_by_num_trees[m] = tree_mod$rmse_oob
}

ggplot(data.frame(num_trees = num_trees_values, oob_rmse = oob_se_bagged_trees_mod_by_num_trees)) + 
  aes(x = num_trees, y = oob_rmse) +
  geom_point() + 
  geom_line()
```

Find the bootstrap s_e for a RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can calculate oob residuals via `e_oob = y_train - rf_mod$predicted`. Plot.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
for (m in 1 : length(num_trees_values)){
  tree_mod = YARF(diamonds_train[, -7], diamonds_train[, 7, drop = TRUE], num_trees = num_trees_values[m])
  oob_se_rf_mod_by_num_trees[m] = tree_mod$rmse_oob
}

ggplot(data.frame(num_trees = num_trees_values, oob_rmse = oob_se_rf_mod_by_num_trees)) + 
  aes(x = num_trees, y = oob_rmse) +
  geom_point() + 
  geom_line()
```

What is the percentage gain / loss in performance of the RF model vs bagged trees model for each number of trees? Gains are negative (as in lower oos s_e).

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Why was this the result?

#TODO

Plot oob s_e by number of trees for both RF and bagged trees by creating a long data frame from the two results.

```{r}
#TO-DO
```

Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum. That maximum will be the number of features assuming that we do not binarize categorical features if you are using `randomForest` or the number of features assuming binarization of the categorical features if you are using `YARF`. Calculate oob s_e for all mtry values.

```{r}
oob_se_by_mtry = array(NA, ncol(diamonds_train))
#TO-DO
```

Plot oob s_e by mtry.

```{r}
#TO-DO
```

Take a sample of n = 2000 observations from the adult data and name it `adult_sample`. Then impute missing values using missForest (we will cover what this is later in.

```{r}
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
adult_train = adult %>% 
  sample_n(2000)
adult_train = missForest(adult_train)$ximp
```


Using the adult_train data, find the bootstrap misclassification error for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

Using the adult_train data, find the bootstrap misclassification error for an RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

What is the percentage gain / loss in performance of the RF model vs bagged trees model?

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Build RF models on adult_train for 500 trees using different `mtry` values: 1, 2, ... the maximum (see above as maximum is defined by the specific RF algorithm implementation). 

```{r}
oob_se_by_mtry = array(NA, ncol(adult_train))
#TO-DO
```


Plot bootstrap misclassification error by `mtry`.

```{r}
#TO-DO
```

Is `mtry` an important hyperparameter to optimize when using the RF algorithm? Explain

#TO-DO

Identify the best model among all values of `mtry`. Fit this RF model. Then report the following oob error metrics: misclassification error, precision, recall, F1, FDR, FOR and compute a confusion matrix.

```{r}
#TO-DO
```

Is this a good model? (yes/no and explain).

#TO-DO

There are probability asymmetric costs to the two types of errors. Assign two costs below and calculate oob total cost.

```{r}
fp_cost = 
fn_cost = 
#TO-DO
```

# Missing Data

Load up the Boston Housing Data and separate into matrix `X` for the features and vector `y` for the response. Randomize the rows

```{r}
rm(list = ls())
set.seed(1)
boston = MASS::Boston
boston_shuffled = boston[sample(1 : nrow(boston)), ]
X = as.matrix(boston_shuffled[, 1 : 13])
y = boston_shuffled$medv
rm(boston, boston_shuffled)
```



Similar to lab 1, write a function that takes a matrix and punches holes (i.e. sets entries equal to `NA`) randomly with an argument `prob_missing`.

```{r}
punch_holes = function(mat, prob_missing){
  n = nrow(mat) * ncol(mat)
  is_missing = as.logical(rbinom(n, 1, prob_missing))
  mat[is_missing] = NA
  mat
}
```

Create a matrix `Xmiss` which is `X` but has missingness with probability of 10% using the function you just wrote. 

```{r}
#TO-DO
```

Also, generate the M matrix and delete columns that have no missingness.

```{r}
M = apply(is.na(Xmiss), 2, as.numeric)
colnames(M) = paste("is_missing_", colnames(X), sep = "")
M = M[, colSums(M) > 0]
```

Split the first 400 observations were the training data and the remaining observations are the test set. For Xmiss, cbind on the M so the model has a chance to fit on "is missing" as we discussed in class.

```{r}
train_idx = 1 : 400
test_idx = setdiff(1 : nrow(X), train_idx)
X_train =     X[train_idx, ]
Xmiss_train = cbind(Xmiss, M)[train_idx, ]
y_train =     y[train_idx]
X_test =      X[test_idx, ]
Xmiss_test =  cbind(Xmiss, M)[test_idx, ]
y_test =      y[test_idx]
```

Fit a random forest model of `y_train ~ X_train`, report oos s_e (not oob) on `X_test`. This ignores missingness

```{r}
#TO-DO
sqrt(mean((y_hat_test - y_test)^2))
```

Impute the missingness in `Xmiss` using the feature averages to create a matrix `Ximp_naive_train` and `Ximp_naive_test`. 

```{r}
#TO-DO
```

Fit a random forest model of `y_train ~ Ximp_naive_train`, report oos s_e (not oob) on `Ximp_naive_test`.

```{r}
#TO-DO
sqrt(mean((y_hat_test - y_test)^2))
```

How much predictive performance was lost due to missingness when naive imputation was used vs when there was no missingness?

```{r}
#TO-DO
```

Use `missForest` to impute the missing entries to create a matrix `Ximp_MF_train` and `Ximp_MF_test`.

```{r}
pacman::p_load(missForest)
#TO-DO
```

Fit a random forest model of `y_train ~ Ximp_MF_train`, report oos s_e (not oob) on `Ximp_MF_test`.

```{r}
#TO-DO
sqrt(mean((y_hat_test - y_test)^2))
```

How much predictive performance was lost due to missingness when `missForest` imputation was used?

```{r}
#TO-DO
```

Why did `missForest` imputation perform better than naive imputation?

#TO-DO

Reload the feature matrix:

```{r}
rm(list = ls())
X = as.matrix(MASS::Boston[, 1 : 13])
```
