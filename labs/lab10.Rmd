---
title: "Lab 10"
author: "Your Name Here"
output: pdf_document
---

#YARF

For the next couple of labs, I want you to make some use of a package I wrote that offers convenient and flexible tree-building and random forest-building. Make sure you have a JDK installed first

https://www.oracle.com/java/technologies/downloads/

Then try to install rJava

```{r}
options(java.parameters = "-Xmx8000m")
pacman::p_load(rJava)
.jinit()
```

If you have error, messages, try to google them. Everyone has trouble with rJava!

If that worked, please try to run the following which will install YARF from my github:

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
pacman::p_load(YARF)
```

Please try to fix the error messages (if they exist) as best as you can. I can help on slack.


#Bagged Trees and Random Forest

Take a training sample of n = 2000 observations from the diamonds data.

```{r}
rm(list = ls())
pacman::p_load(tidyverse)
set.seed(1)
diamonds_train = ggplot2::diamonds %>% 
  sample_n(2000)
```


Using the diamonds data, find the oob s_e for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can create the bagged tree model via setting an argument within the RF constructor function. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

Find the bootstrap s_e for a RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can calculate oob residuals via `e_oob = y_train - rf_mod$predicted`. Plot.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

What is the percentage gain / loss in performance of the RF model vs bagged trees model for each number of trees? Gains are negative (as in lower oos s_e).

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Why was this the result?

#TODO

Plot oob s_e by number of trees for both RF and bagged trees by creating a long data frame from the two results.

```{r}
#TO-DO
```

Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum. That maximum will be the number of features assuming that we do not binarize categorical features if you are using `randomForest` or the number of features assuming binarization of the categorical features if you are using `YARF`. Calculate oob s_e for all mtry values.

```{r}
oob_se_by_mtry = array(NA, ncol(diamonds_train))
#TO-DO
```

Plot oob s_e by mtry.

```{r}
#TO-DO
```

Take a sample of n = 2000 observations from the adult data and name it `adult_sample`. Then impute missing values using missForest.

```{r}
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
adult_train = adult %>% 
  sample_n(2000)
adult_train = missForest(adult_train)$ximp
```


Using the adult_train data, find the bootstrap misclassification error for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

Using the adult_train data, find the bootstrap misclassification error for an RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
```

What is the percentage gain / loss in performance of the RF model vs bagged trees model?

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Build RF models on adult_train for 500 trees using different `mtry` values: 1, 2, ... the maximum (see above as maximum is defined by the specific RF algorithm implementation). 

```{r}
oob_se_by_mtry = array(NA, ncol(adult_train))
#TO-DO
```


Plot bootstrap misclassification error by `mtry`.

```{r}
#TO-DO
```

Is `mtry` an important hyperparameter to optimize when using the RF algorithm? Explain

#TO-DO

Identify the best model among all values of `mtry`. Fit this RF model. Then report the following oob error metrics: misclassification error, precision, recall, F1, FDR, FOR and compute a confusion matrix.

```{r}
#TO-DO
```

Is this a good model? (yes/no and explain).

#TO-DO

There are probability asymmetric costs to the two types of errors. Assign two costs below and calculate oob total cost.

```{r}
fp_cost = 
fn_cost = 
#TO-DO
```

# Asymmetric Cost Modeling, ROC and DET curves

Fit a logistic regression model to the adult_train missingness-imputed data.

```{r}
rm(list = setdiff(ls(), "adult_train"))
#TO-DO
```

Use the function from class to calculate all the error metrics (misclassification error, precision, recall, F1, FDR, FOR) for the values of the probability threshold being 0.001, 0.002, ..., 0.999 in a tibble (dplyr data frame).

```{r}
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
  p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
  misclassification_error = NA, 
  precision = NA, 
  recall = NA, 
  F1 = NA, 
  FDR = NA, 
  FOR = NA
)
#TO-DO
```

Calculate the column `total_cost` and append it to this data frame via `mutate`.

```{r}
#TO-DO
```

Which is the lowest total cost? What is the "winning" probability threshold value providing that minimum total cost?

```{r}
#TO-DO
```

Plot an ROC curve and interpret.

```{r}
#TO-DO
```

#TO-DO interpretation

Calculate AUC and interpret.

```{r}
#TO-DO
```

#TO-DO interpretation

Plot a DET curve and interpret.

```{r}
#TO-DO
```

#TO-DO interpretation

