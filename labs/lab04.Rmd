---
title: "Lab 4 MATH 342W"
author: "Your Name Here"
output: pdf_document
date: "11:59PM March 2"
---

Let's load up the Galton data again
```{r}
pacman::p_load(HistData)
data(Galton)
```

If you were predicting child height from parent and you were using the null model, what would the Rsq be of this model? Compute from first principles.

```{r}
#TO-DO
```

Note that in Math 241 you learned that the sample average is an estimate of the "mean", the population expected value of height. We will call the average the "mean" going forward since it is probably correct to the nearest tenth of an inch with this amount of data.

Run a linear model attempting to explain the childrens' height using the parents' height. Use `lm` and use the R formula notation. Compute and report RMSE and R^2. 

```{r}
#TO-DO
```

What percentage of the variance was explained?

#TO-DO

Is this percentage of the variance explained "good"?

#TO-DO

Find a 95\% reasonable interval for a child's height who's mother-father-average height is 70in.

```{r}
#TO-DO
```

Now convert the units from inches to feet in the dataset D. Run the regression again and report RMSE and R^2

```{r}
#TO-DO
```

Why did RMSE change but Rsq did not change?

#TO-DO


Create a dataset D which we call `Xy` such that the linear model has R^2 about 50\% and RMSE approximately 1.

```{r}
x = #TO-DO
y = #TO-DO
Xy = data.frame(x = x, y = y)
```

Extra credit but required for masters students: create a dataset D and a model that can give you R^2 arbitrarily close to 1 i.e. approximately 1 - epsilon but RMSE arbitrarily high i.e. approximately M.

```{r}
epsilon = 0.01
M = 1000
#TO-DO
```


Create a dataset D which we call `Xy` such that the linear model has R^2 about 0\% but x, y are clearly dependent.

```{r}
x = #TO-DO
y = #TO-DO

#first check that Rsq is around zero
summary(lm(y ~ x))$r.squared
#now check dependence AKA "association" visually
ggplot(data.frame(x = x, y = y)) + geom_point(aes(x = x, y = y))
```

Write a function `my_ols` that takes in `X`, a matrix with with p columns representing the feature measurements for each of the n units, a vector of n responses `y` and returns a list that contains the `b`, the p+1-sized column vector of OLS coefficients, `yhat` (the vector of n predictions), `e` (the vector of n residuals), `df` for degrees of freedom of the model, `SSE`, `SST`, `MSE`, `RMSE` and `Rsq` (for the R-squared metric). Internally, you cannot use `lm` or any other package; it must be done manually. You should throw errors if the inputs are non-numeric or not the same length. Or if `X` is not otherwise suitable. You should also name the class of the return value `my_ols` by using the `class` function as a setter. No need to create ROxygen documentation here.


```{r}
my_ols = function(X, y){
  #TO-DO
}
```

Verify that the OLS coefficients for the `Type` of cars in the cars dataset gives you the same results as we did in class (i.e. the ybar's within group). 

```{r}
#TO-DO
```


Create a prediction method `g` that takes in a vector `x_star` and the dataset D i.e. `X` and `y` and returns the OLS predictions. Let `X` be a matrix with with p columns representing the feature measurements for each of the n units

```{r}
g = function(x_star, X, y){
  #TO-DO
}
```


Load up the famous iris dataset. We are going to do a different prediction problem. Imagine the only input x is Species and you are trying to predict y which is Petal.Length. A reasonable prediction is the average petal length within each Species. Prove that this is the OLS model by fitting an appropriate `lm` and then using the predict function to verify.

```{r}
data(iris)
```

Construct the design matrix with an intercept, X without using `model.matrix`.

```{r}
#TO-DO
```

We now load the diamonds dataset. Skim the dataset using skimr or summary. What is the datatype of the color feature?


```{r}
rm(list = ls())
pacman::p_load(ggplot2, skimr)
diamonds = ggplot2::diamonds
#TO-DO
```

Find the levels of the color feature.

```{r}
levels(diamonds$color)
```

Create new feature in the diamonds dataset, `color_as_numeric`, which is color expressed as a continuous interval value. 

```{r}
#TO-DO
```

Use that converted feature as the one predictor in a regression. How well does this regression do as measured by RMSE?

```{r}
#TO-DO
```


Create new feature in the diamonds dataset, `color_as_nominal`, which is color expressed as a nominal categorical variable. 

```{r}
#TO-DO
```

Use that converted feature as the one predictor in a regression. How well does this regression do as measured by RMSE?

```{r}
#TO-DO
```

Which regression does better - `color_as_numeric` or `color_as_nominal`? Why?

#TO-DO

Now regress both `color_as_numeric` and `color_as_nominal` in a regression. Does this regression do any better (as gauged by RMSE) than either color_as_numeric` or `color_as_nominal` alone?

```{r}
#TO-DO
```

What are the coefficients (the b vector)? 

```{r}
#TO-DO
```

Something appears to be anomalous in the coefficients. What is it? Why?

#TO-DO

Return to the iris dataset. Find the hat matrix H for this regression.

```{r}
rm(list = ls())
#TO-DO
```

Verify this hat matrix is symmetric using the `expect_equal` function in the package `testthat`.

```{r}
#TO-DO
```

Verify this hat matrix is idempotent using the `expect_equal` function in the package `testthat`.

```{r}
pacman::p_load(testthat)
#TO-DO
```

Using the `diag` function, find the trace of the hat matrix.

```{r}
#TO-DO
```

It turns out the trace of a hat matrix is the same as its rank! But we don't have time to prove these interesting and useful facts..

For masters students: create a matrix X-perpendicular.

```{r}
#TO-DO
```

Using the hat matrix, compute the yhat vector and using the projection onto the residual space, compute the e vector and verify they are orthogonal to each other.

```{r}
#TO-DO
```

Compute SST, SSR and SSE and R^2 and then show that SST = SSR + SSE.

```{r}
#TO-DO
```

Find the angle theta between y - ybar 1 and yhat - ybar 1 and then verify that its cosine squared is the same as the R^2 from the previous problem.

```{r}
#TO-DO
```

