---
title: "Practice Lecture 19 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---

# C++ and R

R goes back to 1995 when it was adapted from S (written in 1976 by John Chambers at Bell Labs) with minor modifications. The core of base R is written in C and Fortran. These two languages are the fastest known languages (how to measure "fastest" is a huge debate). Thus, many common functions in base R are very fast. For instance the `sort` function is as fast as C/Fortran since it immediately calls compiled C/Fortran routines.

However, R code itself that you write is "interpreted" which means it is not compiled until you run it. And it has to compile on-the-fly, making it very slow. Prior to v3.4 (April, 2017) it was even slower since the code wasn't JIT compiled. All this "real CS" stuff you can learn in another class...

We will also demo bridges between Java<=>R as well as Python<=>R but you only be responsible for the C++<=>R bridge as this really is a good skill I would like you to know for the future.

One notable place to observe this slowness relative to other languages is in looping. For example:

```{r}
SIZE = 1e6
v = 1 : SIZE
```

Take for example a simple function that computes square roots on each element

```{r}
sqrt_vector = function(v){
  v_new = array(NA, length(v))
  for (i in 1 : length(v)){
    v_new[i] = sqrt(v[i])
  }
  v_new
}
```

How fast does this run? Let's use a cool package called `microbenchmark` that allows us to do an operation many times and see how long it takes each time to get an average:

```{r}
pacman::p_load(microbenchmark)
microbenchmark(
  sqrt_vector(v), 
  times = 10
)
```

Does the apply function help?

```{r}
microbenchmark(
  apply(v, 1, FUN = sqrt), 
  times = 10
)
```

Strange that this takes so long? So it doesn't help... it hurts A LOT. Unsure why... Be careful with apply! 

How much faster in C++ should this be?

Enter the `Rcpp` package - a way to compile little bits (or lotta bits) of C++ on the fly.

```{r}
pacman::p_load(Rcpp)
```


Let's write this for loop function to sqrt-ize in C++. We then  compile it and then save it into our namespace to be called like a regular function. Note that we use C++ classes that are not part of standard C++ e.g. "NumericVector". Rcpp comes build in with classes that are interoperable with R. It's not hard to learn, just takes a small dive into the documentation.

```{r}
cppFunction('
  NumericVector sqrt_vector_cpp(NumericVector v) {
    int n = v.size();
    NumericVector v_new(n);
    for (int i = 0; i < n; i++) { //indices from 0...n-1 not 1...n!
      v_new[i] = sqrt(v[i]);
    }
    return v_new;
  }
')
```

What do these two functions look like?

```{r}
sqrt_vector
sqrt_vector_cpp
```

The first one shows the R code and then says it is bytecode-compiled which means there are speedups used in R (go to an advanced CS class) but we will see these speedups aren't so speedy! The other just says we `.Call` some C++ function in a certain address (pointer) and the argument to be inputted.

What is the gain in runtime?

```{r}
microbenchmark(
  sqrt_vector_cpp(v), 
  times = 10
)
```

WOW. 10x!!! Can't beat that with a stick...

Let's do a not-so-contrived example...

Matrix distance... Let's compute the distances of all pairs of rows in a dataset. I will try to code the R as efficiently as possible by using vector subtraction so there is only two for loops. The C++ function will have an additional loop to iterate over the features in the observations.

```{r}
#a subset of the diamonds data
SIZE = 1000
X_diamonds = as.matrix(ggplot2::diamonds[1 : SIZE, c("carat", "depth", "table", "x", "y", "z")])

compute_distance_matrix = function(X){
  n = nrow(X)
  D = matrix(NA, n, n)
  for (i_1 in 1 : (n - 1)){
    for (i_2 in (i_1 + 1) : n){
      D[i_1, i_2] = sqrt(sum((X[i_1, ] - X[i_2, ])^2))
    }
  }
  D
}

cppFunction('
  NumericMatrix compute_distance_matrix_cpp(NumericMatrix X) {
    int n = X.nrow();
    int p = X.ncol();
    NumericMatrix D(n, n);
    std::fill(D.begin(), D.end(), NA_REAL);

    for (int i_1 = 0; i_1 < (n - 1); i_1++){
      //Rcout << "computing for row #: " << (i_1 + 1) << "\\n";
      for (int i_2 = i_1 + 1; i_2 < n; i_2++){
        double sqd_diff = 0;
        for (int j = 0; j < p; j++){
          sqd_diff += pow(X(i_1, j) - X(i_2, j), 2); //by default the cmath library in std is loaded
        }
        D(i_1, i_2) = sqrt(sqd_diff); //by default the cmath library in std is loaded
      }
    }
    return D;
  }
')
```

```{r}
microbenchmark(
  {D = compute_distance_matrix(X_diamonds)},
  times = 10
)

round(D[1 : 5, 1 : 5], 2)
```

Slow...

```{r}
microbenchmark(
  {D = compute_distance_matrix_cpp(X_diamonds)},
  times = 10
)
round(D[1 : 5, 1 : 5], 2)
```

Absolutely lightning... ~200x faster on my laptop than R's runtime.

Writing functions as strings that compile is annoying. It is better to have separate files. For instance...

```{r}
sourceCpp("distance_matrix.cpp")
```

Here are a list of the data structures in Rcpp: https://teuder.github.io/rcpp4everyone_en/070_data_types.html#vector-and-matrix

Another place where C++ pays the rent is recursion. Here is a quicksort implementation in R taken from somewhere on the internet.

```{r}
quicksort_R <- function(arr) {
  # Pick a number at random.
  mid = sample(arr, 1)

  # Place-holders for left and right values.
  left = c()
  right = c()
  
  # Move all the smaller values to the left, bigger values to the right.
  lapply(arr[arr != mid], function(d) {
    if (d < mid) {
      left <<- c(left, d) #needs to assign to the global variable here to jump out of the scope of the apply function
    }
    else {
      right <<- c(right, d) #needs to assign to the global variable here to jump out of the scope of the apply function
    }
  })
  
  if (length(left) > 1) {
    left = quicksort_R(left)
  }
  
  if (length(right) > 1) {
    right = quicksort_R(right)
  }
  
  # Finally, return the sorted values.
  c(left, mid, right)
}
```

Let's create a random array to test these sorts on:

```{r}
n = 10000
x = rnorm(n)
```


Let's profile the pure R sort function:

```{r}
microbenchmark(
  x_sorted_pure_R = quicksort_R(x),
  times = 10
)
```

Let's profile R's `sort` function.

```{r}
microbenchmark(
  x_sorted_base_R = sort(x),
  times = 10
)
```

Let's just ensure our method worked...

```{r}
x_sorted_pure_R = quicksort_R(x)
x_sorted_base_R = sort(x)
pacman::p_load(testthat)
expect_equal(x_sorted_pure_R, x_sorted_base_R)
```

Basically infinitely faster. Let's make our own C++ implementation.

```{r}
sourceCpp("quicksort.cpp")
```

and profile it:

```{r}
microbenchmark(
  x_sorted_cpp = quicksort_cpp(x),
  times = 10
)
```

Let's just ensure this method worked...

```{r}
pacman::p_load(testthat)
expect_equal(x_sorted_cpp, x_sorted_base_R)
```

Why is our C++ slower than `sort`. Because `sort` is also in C++ or Fortran and it's been likely optimized and reoptimized up to wazoo for decades. Also, Rcpp's data structures may be slower than base R's data structures. There may be some speed lost to translating to `NumericVector` from `double[]` or something like that.

Can you call R from Rcpp? You bet:

```{r}
cppFunction('
  NumericVector rnorm_cpp_R(int n, double mean, double sd){
      // get a pointer to R\'s rnorm() function
      Function f("rnorm");   
  
      // Next code is interpreted as rnorm(n, mean, sd)
      return f(n, Named("sd")=sd, _["mean"]=mean);
  }
')

rnorm_cpp_R(5, 1, .01)
```

A few math functions are implemented for you already:

```{r}
evalCpp('R::qnorm(0.5, 0, 1, 1, 0)')
evalCpp('R::qnorm(0.5, 0, 1)') #BOOM
```

Further, there are many common functions that are already wrapped for you via "Rcpp-sugar" which was the Rcpp's author's attempt to make Rcpp a whole lot easier, see [here](http://dirk.eddelbuettel.com/code/rcpp/Rcpp-sugar.pdf).

```{r}
evalCpp('rnorm(10, 100, 3)')
```

If you want blazing fast linear algebra, check out package `RcppArmadillo` which is a wrapper around Apache's Armadillo (namespace is "arma" in the code), an optimized linear algebra package in C++. Here is an example taken from [here](https://scholar.princeton.edu/sites/default/files/q-aps/files/slides_day4_am.pdf). It involves solving for b-vec in a standard OLS.

```{r}
pacman::p_load(RcppArmadillo)

cppFunction('
  arma::mat ols_cpp(arma::mat X, arma::mat y){
    arma::mat Xt = X.t();
    return solve(Xt * X, Xt * y);
  }
', depends = "RcppArmadillo")

n = 500
Xy = data.frame(int = rep(1, n), x1 = rnorm(n), x2 = rnorm(n), x3 = rnorm(n), y = rnorm(n))
X = as.matrix(Xy[, 1 : 4])
Xt = t(X)
y = as.matrix(Xy[, 5])

#does the function work?
expect_equal(as.numeric(ols_cpp(X, y)), as.numeric(solve(Xt %*% X) %*% Xt %*% y))
```

Now how fast is it?

```{r}
microbenchmark(
  R_via_lm = lm(y ~ 0 + ., data = Xy),
  R_via_lm_fit = lm.fit(X, y),
  R_matrix_multiplication = solve(Xt %*% X) %*% Xt %*% y,
  cpp_with_armadillo = ols_cpp(X, y),
    times = 100
)
```

About 4x faster than R's optimized linear algebra routines. Supposedly it can go even faster if you enable parallelization within Armadillo. I couldn't get that demo to work...

Note lm is slow because it does all sorts of other stuff besides computing b-vec e.g. builds the model matrix, computes Rsq, computes residuals, does statistical testing, etc...

Here are the places where Rcpp is recommended to be used (from https://teuder.github.io/rcpp4everyone_en/010_Rcpp_merit.html)

* Loop operations in which later iterations depend on previous iterations.
* Accessing each element of a vector/matrix.
* Recurrent function calls within loops.
* Changing the size of vectors dynamically.
* Operations that need advanced data structures and algorithms (we don't do this in this class).

If you want even faster you can parallelize with OpenMP within Rcpp a la https://mfasiolo.github.io/sc2-2019/rcpp_advanced_iii/1_openmp/.

# Java and R

We just did C++ with R. Is there a bridge to Java? Yes (and there's bridges to many other languages too). Java and R can speak to each other through proper configuration of the `rJava` package. You need to have a full JDK of Java installed on your computer and have its binary executables in the proper path. This demo will be in Java JDK 8 (released in 2014 and not officially supported after 2020) since I haven't tested on the more modern Java JDK's yet. We first install `rJava` if necessary:

```{r}
if (!pacman::p_isinstalled(rJava)){
  pacman::p_load(pkgbuild)
  if (pkgbuild::check_build_tools()){
    install.packages("rJava", type = "source")
  }
  install.packages("rJava")
}
```

Now we load the package. Before we do, we set the JVM to have 8GB of RAM. After we load it, we initialize te JVM. This should print out nothing or "0" to indicate success.

```{r}
options(java.parameters = "-Xmx8g")
pacman::p_load(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
```

Just like the whole `Rcpp` demo, we can do a whole demo with `rJava`, but we won't. Here's just an example of creating a Java object and running a method on it:

```{r}
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue") #java_double.intValue();
#call a static method
J("java/lang/String", "valueOf", java_double)
```

A note on rJava vs Rcpp. 

* If you're doing quick and dirty fast functions for loops and recursion, do it in Rcpp since there is lower overhead of programming. 
* If you are programming more full-featured software, go with rJava. 
* Also, if you need full-featured parallelized execution and threading control e.g. thread pooling and the ease of debugging, my personal opinion is that rJava is easier to get working with less dependencies. Rcpp threading is trickier and so is the openMP directives within Rcpp.
* Further, the JVM is fully asynchronous which means it runs completely independently of R. What this means is that you can execute something in Java, Java can "thread it off" and return you to the R prompt with a pointer to the object that houses its execution. You can then query the object. We will see demos of this.


# Python and R

No demo would be complete without this. 

```{r}
pacman::p_load(reticulate)
py_available()
py_numpy_available()

# import numpy and specify no automatic Python to R conversion
np = import("numpy", convert = FALSE)

# do some array manipulations with NumPy
python_arr = np$array(1 : 4)
cumsum_python = python_arr$cumsum()
class(cumsum_python)
cumsum_python

# convert to R explicitly at the end
cumsum_R = py_to_r(cumsum_python)
cumsum_R

# now do the opposite, start with R and convert to Python
r_to_py(cumsum_R)
r_to_py(as.integer(cumsum_R))
```

Let's look at an example of Python Data Analysis Library (pandas). Let's install if not already installed:

```{r}
import("pandas", convert = FALSE)
# py_install("pandas")
```

And python even works in Rstudio's markdown files e.g.

```{python}
#this is python code!!!
import pandas as pd
flights = pd.read_csv("https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv")
flights = flights.dropna()
flights.columns
flights[flights['DEST_AIR'] == "JFK"]
```

And then switch back to R and have access to the object we instantiated in python via the `py` object:

```{r}
#this is R code!!!
ggplot(py$flights) + 
  aes(x = AIRLINE, y = ARR_DELAY) +
  geom_boxplot()

lm(ARR_DELAY ~ AIRLINE, py$flights)
```



# Regression Trees

Let's fit a regression tree. We will use the development package `YARF` which I've been hacking on now for a few years. The package internals are written in Java which we just installed above. Since `YARF` is not on CRAN, we install the package from my github including its dependency (if necessary) and then load it 

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
```

The data will be fitting with the regression tree is a sine curve plus noise:

```{r}
pacman::p_load(tidyverse, magrittr)
n = 500
x_max = 4 * pi
x = runif(n, 0, x_max)
sigma = 0.05
y = ifelse(x < 2 * pi, 0.5 * sin(x), 0) + rnorm(n, 0, sigma)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd = 0.6) 
```

Now we fit a regression tree to this model. Nevermind the `calculate_oob_error` argument for now. This will be clear why FALSE is NOT the default soon enough.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, calculate_oob_error = FALSE)
```

How "big" is this tree model? How many df? It's the number of leaf nodes (final nodes).

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

What does $g(x)$ look like?

```{r}
Nres = 1000
x_predict = data.frame(x = seq(0, x_max, length.out = Nres))
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Obviously overfit - but not that bad... let's try lowering the complexity by stopping the tree construction at a higher node size.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 50, calculate_oob_error = FALSE)
yhat = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = yhat), col = "blue")
```

Less overfitting now but now it's clearly underfit! We can play with the nodesize. Or we can use the train-select-test split meta algorithm to pick the model (the nodesize). What if nodesize = 1?

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 1, calculate_oob_error = FALSE)
yhat = predict(tree_mod, data.frame(x = x))
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x, y = yhat), col = "blue")
```
What's the error?

```{r}
sum((y - yhat)^2) #SSE
```

Are we sure we have a leaf node for each observation?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

In contrast, what if nodesize > n?

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = n + 1, calculate_oob_error = FALSE)
yhat = predict(tree_mod, data.frame(x = x))
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x, y = yhat), col = "blue")
unique(yhat)
mean(y)
```

Then you never do any splits, so yhat = ybar, the null model.

Are we sure we have one root node?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's try this using oos validation and trace out a performance curve to find the optimal hyperparameter.

```{r}
K = 4
set.seed(1984)
select_idx = sample(1 : n, round(1 / K * n))
x_select = x[select_idx]
y_select = y[select_idx]
train_idx = setdiff(1 : n, select_idx)
x_train = x[train_idx]
y_train = y[train_idx]
n_train = length(train_idx)

max_nodesize_to_try = 70
in_sample_errors = array(NA, max_nodesize_to_try)
oos_errors = array(NA, max_nodesize_to_try)
for (i in 1 : max_nodesize_to_try){
  tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = i, calculate_oob_error = FALSE)
  yhat = predict(tree_mod, data.frame(x = x_train))
  in_sample_errors[i] = sd(y_train - yhat)
  yhat_select = predict(tree_mod, data.frame(x = x_select))
  oos_errors[i] = sd(y_select - yhat_select)
}

ggplot(data.frame(nodesize = 1 : max_nodesize_to_try, in_sample_errors = in_sample_errors, oos_errors = oos_errors)) + 
  geom_point(aes(nodesize, in_sample_errors), col = "red") +
  geom_point(aes(nodesize, oos_errors), col = "blue")
```

Looks like optimal nodesize is...

```{r}
which.min(oos_errors)
```

You're used to seeing complexity increase on the x-axis, so we can just invert the nodesize:

```{r}
ggplot(data.frame(inv_nodesize = (1 : max_nodesize_to_try)^-1, in_sample_errors = in_sample_errors, oos_errors = oos_errors)) + 
  geom_point(aes(inv_nodesize, in_sample_errors), col = "red") +
  geom_point(aes(inv_nodesize, oos_errors), col = "blue") + 
  scale_x_log10()
```

For some reason, we do not see serious overfitting even at nodesize = 1. Why? It's because the way trees predict - they always stay within [y_min, y_max]. There is no Runge phenomenon or line extensions so it really limits how bad your errors can be at the edges especially.

Optimal tree

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = which.min(oos_errors), calculate_oob_error = FALSE)
yhat = predict(tree_mod, data.frame(x = x))
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x, y = yhat), col = "blue")
```

# Regression Trees with Real Data

Now let's look at a regression tree model predicting medv in the Boston Housing data. We first load the data and do a training-test split:

```{r}
rm(list = ls())
set.seed(1984)
pacman::p_load(MASS)
data(Boston)
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
n_train = nrow(X_train)
```

And fit a tree model. The default hyperparameter, the node size is $N_0 = 5$.

```{r}
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, X_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
sd(y_train - linear_mod$fitted.values)
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

Immediately we're finding the most important variables and then interacting them.

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, X_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

The take-home message here is that the tree beats the linear model in future predictive performance but the only way to be truly convinced of this is to do the split over and over to get a sense of the average over the massive variability (like the previous demo) or to do CV to reduce the error of the estimate. 

Why does the regression tree beat the linear model? Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

We have 161 degrees of freedom - that's a lot of interactions and non-linearity flexibility.

About how many observations are in each leaf on average?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

How many per leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why is it not exactly 1 on average? I think it's because...

```{r}
data.table::uniqueN(y_train)
length(y_train)
```

Regardless of this point, this model is essentially giving each observation it's own y-hat, it's own personal guess which will be its own personal y. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, X_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

This is the expected behavior in perfect fitting.

```{r}
y_hat_test_tree = predict(tree_mod, X_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

It overfits but amazing it doesn't get clobbered completely! And its results are on-par with the non-overfit linear model probably because it made up for the overfitting by reducing misspecification error. 

Trees are truly amazing!

# Extrapolation with Trees

```{r}
rm(list = ls())
pacman::p_load(HistData, ggplot2)
data(Galton)

tree_mod = YARFCART(data.frame(x = Galton$parent), Galton$child, calculate_oob_error = FALSE)

mod = lm(child ~ parent, Galton)
b_0 = mod$coefficients[1]
b_1 = mod$coefficients[2]
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue", linewidth = 1) +
  xlim(63.5, 72.5) + 
  ylim(63.5, 72.5) +
  coord_equal(ratio = 1)

degree_2_poly_mod = lm(child ~ poly(parent, 2, raw = TRUE), Galton)
b_poly_2 = coef(degree_2_poly_mod)
degree_13_poly_mod = lm(child ~ poly(parent, 13, raw = TRUE), Galton)
b_poly_13 = coef(degree_13_poly_mod)
b_poly_13[is.na(b_poly_13)] = 0

plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}
plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}
plot_YARFCART = function(x, tree_mod){
  predict(tree_mod, data.frame(x = x))
}

xymin = 40
xymax = 90
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue") +
  coord_cartesian(xlim = c(xymin, xymax), ylim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_2, args = list(b = b_poly_2), col = "red", xlim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "orange", xlim = c(xymin, xymax)) +
  stat_function(fun = plot_YARFCART, args = list(tree_mod = tree_mod), col = "purple", xlim = c(xymin, xymax))
```



# Classification Trees

Let's get the cancer biopsy data:

```{r}
rm(list = ls())
pacman::p_load(YARF, tidyverse, magrittr)
data(biopsy, package = "MASS")
biopsy %<>% na.omit %>% dplyr::select(-ID) #for some reason the "select" function is scoping elsewhere without this explicit directive
colnames(biopsy) = c(
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
n_train = nrow(X_train)
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree. The default nodesize for classification is 1!

```{r}
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

How many observations per leaf?

```{r}
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, length_in_px_per_half_split = 30, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

This is because the default nodesize is 1 which implies that all nodes are always "pure" meaning they only have the same class.

That's the default for classification tree algorithm. Don't be lazy: you probably should use a train-select-test and optimize nodesize!!!

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

Not bad! What's the null model performance?

```{r}
mean(y_test != "benign") #the modal response is "benign"
```

We do well out of the box. But we can do better if we select nodesize.

Let's do another example with the adult data:


```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
adult$income = factor(adult$income)
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
set.seed(1984)
test_size = 2000

train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
n_train = nrow(X_train)

select_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_select = adult[select_indices, ]
y_select = adult_select$income
X_select = adult_select
X_select$income = NULL

test_indices = sample(setdiff(1 : nrow(adult), c(train_indices, select_indices)), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a default tree (nodesize = 1 since this is classification) and look at the most important splits:

```{r}
tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE)
illustrate_trees(tree_mod, max_depth = 5, length_in_px_per_half_split = 30, open_file = TRUE)
```

How complex is the model and how many observations per node?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

In-sample performance?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

This is to spec for nodesize = 1.

Out of sample g_0 performance?

```{r}
mean(y_test != "<=50K")
```

Out of sample tree performance?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

The warning was legit this time. What's it saying?

Let's do better by using the selection set

```{r}
nodesizes = seq(1, 400, by = 10)
misclassification_error_by_nodesize = array(NA, length(nodesizes))
for (i in 1 : length(nodesizes)){
  tree_mod = YARFCART(X_train, y_train, calculate_oob_error = FALSE, nodesize = nodesizes[i])  
  y_hat_select = predict(tree_mod, X_select)
  misclassification_error_by_nodesize[i] = mean(y_select != y_hat_select)
}
ggplot(data.frame(misc_err = misclassification_error_by_nodesize, nodesize = nodesizes)) + 
  geom_point(aes(x = nodesize, y = misc_err))
```

The optimal nodesize is faaaar from 1. Best performance has nodesize of:

```{r}
nodesizes[which.min(misclassification_error_by_nodesize)]
```

How does it do on the test set?

```{r}
tree_mod = YARFCART(rbind(X_train, X_select), c(y_train, y_select), calculate_oob_error = FALSE, nodesize = nodesizes[which.min(misclassification_error_by_nodesize)])
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

This is actually decent performance (compared to what we've seen earlier in the semester).

Note: these CART demos have been purely academic since I doubt people still use CART in production models that require best possible accuracy these days since this issue of nodesize selection was fixed with bagging (we will get to this soon) and Random Forests (we'll get to this soon as well) and these methods also allow us to get closer to f(x). So you can think of the CART algorithm as an intermediate step to get to the "real stuff".

However, people do still use CART for interpretation because the base of the tree shows the important variables and interactions. So it gives an idea about how f(x) works. The whole topic of "interpretability" vs "predictive performance" is a very hot topic now but we just don't have time to delve into it more. Basically, the higher the predictive performance, the lower the interpretability.

