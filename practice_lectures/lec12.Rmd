---
title: "Practice Lecture 12 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---









# Assessing overfitting in practice

Let's examine this again. This time we use one data set which is split between training and testing.

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
p = 50
X = matrix(runif(n * p, xmin, xmax), ncol = p)

#best possible model - only one predictor matters!
h_star_x = beta_0 + beta_1 * X[,1 ]

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

Now we split $\mathbb{D}$ into training and testing. We define $K$ first, the inverse proportion of the test size.

```{r}
K = 5 #i.e. the test set is 1/5th of the entire historical dataset

#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#let's ensure these are all correct
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Now let's fit the model $g$ to the training data and compute in-sample error metrics:

```{r}
mod = lm(y_train ~ ., data.frame(X_train))
summary(mod)$r.squared
sd(mod$residuals)
```

Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:

```{r}
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```



MUCH worse!! Why? We overfit big time...

Can we go back now and fit a new model and see how we did? NO...

So how are we supposed to fix a "bad" model? We can't unless we do something smarter. We'll get there.