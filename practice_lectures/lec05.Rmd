---
title: "Practice Lecture 5 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---
 
# Intro to Optimization in R

You will code a basic SVM for lab. In order to do so, you need to minimize the Vapnik Objective function. We use the `optimx` library. It has many different options for doing optimization including gradient descent (which we studied in 340). Optimization is really the subject of a whole other class which I suggest you all take. It's really important to the AI space especially deep learning.

```{r}
pacman::p_load(optimx)
```

Here's an example of locating the minimum of (w_1 - 3)^2+5 + (w_2 + 4)+12. The minimum is 17 and it occurs when w_1 = 3 and w_2 = -4. We will demonstrate for all implemented optimization methods. Some won't work:

```{r}
wvec_0 = c(0, 0) #the starting values
objective_function = function(wvec){(wvec[1] - 3)^2+5 + (wvec[2] + 4)^2+12}
#runs minimization by default
optimx(wvec_0, objective_function, 
       method = c('Nelder-Mead', 'BFGS', 'CG', 'L-BFGS-B', 'nlm', 'nlminb', 'spg', 'ucminf', 'newuoa', 'bobyqa', 'nmkb', 'hjkb', 'Rcgmin', 'Rvmmin'))
```

Optimization becomes more accurate if we supply the gradient vector and Hessian matrix. This is usually worth it if it's easy to compute but it's usually difficult to compute.

```{r}
objective_function_gradient = function(wvec){c(2 * wvec[1] - 6, 2 * wvec[2] + 8)}
objective_function_hessian = function(wvec){matrix(c(2, 0, 0, 2), nrow = 2)}
optimx(wvec_0, objective_function, gr = objective_function_gradient, hess = objective_function_hessian, method = c('Nelder-Mead', 'BFGS', 'CG', 'L-BFGS-B', 'nlm', 'nlminb', 'spg', 'ucminf', 'newuoa', 'bobyqa', 'nmkb', 'hjkb', 'Rcgmin', 'Rvmmin'))
```

In this case, the function was so ridiculously easy to minimize (being differentiable and of dimension of only 2) we only get improvements in far off decimal places if we give it the first and second derivatives.

For the purposes of the homework, you should use the method "BFGS". In real life, you have to play around and see what happens! No optimization method works well for all situations. Read the documentation and read up on these methods on wikipedia.


## Support Vector Machines (SVM)

Here we use the `e1071` library.

```{r}
pacman::p_load(e1071)
```

We make a simple dataset first.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

And let's plot the data:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Now we fit a linear SVM. As for the cost, which is related to lambda from class (although I'm not entirely sure how exactly it is related), we will leave the default which is 1.

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = 1, scale = FALSE)
```

The model object can be queried to find the "support vectors" i.e. the observations that lie on the wedge. Let's visualize them too.

```{r}
Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)
Xy_simple$is_support_vector = NULL #cleanup
simple_viz_obj
```


Now we calculate the weight vector. This is technical and not covered in the class yet (or maybe never):

```{r}
w_vec_simple_svm = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
w_vec_simple_svm
w_norm = sqrt(sum(w_vec_simple_svm^2))
w_norm
```

We can also plot it. We have to convert from Hesse Normal form back into point-intercept form. Note that $b$ is the first entry of the `w_vec_simple_svm` vector

```{r}
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

We can also plot the wedge by plotting the top line (where b is augmented by 1) and the bottom line (where b is diminished by 1).

```{r}
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```



To understand the hyperparameter, let's introduce another data point so the training data is no longer linearly separable.

```{r}
Xy_simple = rbind(Xy_simple, c(0, 3.2, 3.2))
```

and plot it:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Let's try SVM at different $\lambda$ values.

```{r}
#From documentation:
#"Parameters of SVM-models usually must be tuned to yield sensible results!"

lambda = 0.01
# lambda = 0.1
# lambda = 1
# lambda = 2

Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
summary(svm_model)
w_vec_simple_svm = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
sqrt(sum(w_vec_simple_svm^2))

Xy_simple$is_support_vector = rep("no", n)
Xy_simple$is_support_vector[svm_model$index] = "yes"
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response, shape = is_support_vector)) + 
  geom_point(size = 5)

simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_svm_top_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] + 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_svm_bottom_line = geom_abline(
    intercept = -(w_vec_simple_svm[1] - 1) / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "yellow")
simple_viz_obj + simple_svm_line + simple_svm_top_line + simple_svm_bottom_line
```

What lesson did we learn here? This hyperparameter really matters! We need to figure out a way to deal with selecting the "right" hyperparameter value automatically. So far neither the perceptron nor the SVM is an algorithm for binary classification that comes without flaws.

What are the support vectors now? Any data point that would change the dividing line if removed. It's a more expansive definition than when there was no hinge loss term.

Let's see how the SVM does on the breast cancer data. Once again, the null model gives error rate:

```{r}
rm(list = ls())
Xy = na.omit(MASS::biopsy) #The "breast cancer" data
?MASS::biopsy
head(Xy)
X = as.matrix(Xy[, 2 : 10]) #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
prop.table(table(y_binary))
```

```{r}
svm_model = svm(X, y_binary, kernel = "linear", cost = 0.000025)
w_vec = c(
  -svm_model$rho, #the b term
  t(svm_model$coefs) %*% X[svm_model$index, ] # the other terms
)
yhat = ifelse(cbind(1, X) %*% w_vec > 0, 1, 0)
table(yhat)
sum(y_binary != yhat) / length(y_binary)
```

That did pretty well (after I tried a bunch of cost values for the hyperparameter). We'll see why this is likely a bad idea later in a couple weeks.
