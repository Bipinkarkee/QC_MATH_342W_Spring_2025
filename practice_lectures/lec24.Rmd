---
title: "Practice Lecture 23 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


# Boosting

Nice simple explanation: https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725

The first boosting algorithm was called adaboost:
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=A+decision-theoretic+generalization+of+on-line+learning+and+an+application+to+boosting&btnG=

And then it was improved with gradient boosting and stochastic gradient boosting:
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=Greedy+function+approximation%3A+a+gradient+boosting+machine&btnG=
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=Stochastic+gradient+boosting&btnG=

And then finally with "extreme gradient boosting" which is just a better implementation of gradient boosting with more bells and whistles. I believe is state of the art at the time of this writing:
https://scholar.google.com/scholar?hl=en&as_sdt=7%2C39&q=Xgboost%3A+A+scalable+tree+boosting+system&btnG=

```{r}
options(java.parameters = "-Xmx8000m")
pacman::p_load(xgboost, YARF, tidyverse)
?xgboost
boosting_hyperparams = list(
 eta = 0.3, #default step size
 max_depth = 6, #default
 subsample = 1, #default 
 colsample_bytree = 1, #default
 nthread = 11,
 print_every_n = 250
)
```

Look at performance on the diamonds dataset.

```{r}
set.seed(1)
diamonds = ggplot2::diamonds
diamonds = diamonds %>% 
  mutate(cut = factor(cut, ordered = FALSE)) %>%
  mutate(color = factor(color, ordered = FALSE)) %>%
  mutate(clarity = factor(clarity, ordered = FALSE))
diamonds_mm = model.matrix(price ~ ., diamonds)
train_size = 4000
train_indices = sample(1 : nrow(diamonds), train_size)

diamonds_train = diamonds[train_indices, ]
y_train = diamonds_train$price
X_train = diamonds_train
X_train_mm = diamonds_mm[train_indices, ]
X_train$price = NULL

test_size = 10000
test_indices = sample(setdiff(1 : nrow(diamonds), train_indices), test_size)
diamonds_test = diamonds[test_indices, ]
y_test = diamonds_test$price
X_test = diamonds_test
X_test_mm = diamonds_mm[test_indices, ]
X_test$price = NULL

xgboost_mod = xgboost(
 data = X_train_mm, #X: we need to convert it do a design matrix
 label = y_train, #y
 objective = "reg:squarederror", #SSE for a regression problem
 booster = "gbtree",
 #######hyperparameters
 nrounds = 2500, #M, no default, varies with n (should be cross-validated)
 eta = boosting_hyperparams$eta,
 max_depth = boosting_hyperparams$max_depth,
 subsample = boosting_hyperparams$subsample,
 colsample_bytree = boosting_hyperparams$colsample_bytree,
 nthread = boosting_hyperparams$nthread,
 print_every_n = boosting_hyperparams$print_every_n
)

y_hat_test = predict(xgboost_mod, X_test_mm)
cat("default boosting")
cat("RMSE:", sqrt(mean((y_test - y_hat_test)^2)))


rf_mod = YARF(X_train, y_train, 
              mtry = 4, #I tried a few values of mtry (you should cv)
              num_trees = boosting_hyperparams$nrounds, #same number of trees as boosting
              calculate_oob_error = FALSE #we are using a test set - so no need to do this calculation
          ) 

y_hat_test = predict(rf_mod, X_test)
cat("RF with mtry optimized")
cat("RMSE:", sqrt(mean((y_test - y_hat_test)^2)))
```

Yes - xgboost can beat RF. Does it always? Not so clear. See
https://www.r-bloggers.com/2020/07/is-catboost-the-best-gradient-boosting-r-package/



Look at performance on adult dataset.

```{r}
rm(list = ls()) #reset R here

options(java.parameters = "-Xmx8000m")
pacman::p_load(xgboost, YARF, tidyverse)
boosting_hyperparams = list(
 eta = 0.3, #default step size
 max_depth = 6, #default 
 subsample = 1, #default 
 colsample_bytree = 1, #default
 nthread = 11,
 print_every_n = 100
)

pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
adult_mm = model.matrix(income ~ ., adult)

set.seed(1)
train_size = 2000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = as.numeric(adult_train$income == ">50K")
X_train = adult_train
X_train$income = NULL
X_train_mm = adult_mm[train_indices, ]

test_size = 10000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = as.numeric(adult_test$income == ">50K")
X_test = adult_test
X_test$income = NULL
X_test_mm = adult_mm[test_indices, ]

xgboost_mod = xgboost(
 data = X_train_mm,
 label = y_train,
 objective = "binary:logistic",
 # objective =  "binary:hinge",
 #######hyperparameters
 nrounds = 3000, #M, usually needs to be a little bit bigger per n in classification (again should be cv'd)
 eta = boosting_hyperparams$eta,
 max_depth = boosting_hyperparams$max_depth,
 subsample = boosting_hyperparams$subsample,
 colsample_bytree = boosting_hyperparams$colsample_bytree,
 nthread = boosting_hyperparams$nthread,
 print_every_n = boosting_hyperparams$print_every_n
)

y_hat_test = as.numeric(predict(xgboost_mod, X_test_mm) > 0.5) #default symmetric-cost classification rule when using probability regression
oos_confusion = table(y_test, y_hat_test)
oos_confusion
cat("default boosting")
cat("FDR =", oos_confusion[1, 2] / sum(oos_confusion[, 2]), "\n")
cat("FOR =", oos_confusion[2, 1] / sum(oos_confusion[, 1]), "\n")
cat("miscl err =", (oos_confusion[2, 1] + oos_confusion[1, 2]) / length(y_test), "\n")

rf_mod = YARF(data.frame(X_train), as.factor(y_train))

y_hat_test = as.numeric(predict(rf_mod, data.frame(X_test))) - 1
oos_confusion = table(y_test, y_hat_test)
oos_confusion
cat("default RF")
cat("FDR =", oos_confusion[1, 2] / sum(oos_confusion[, 2]), "\n")
cat("FOR =", oos_confusion[2, 1] / sum(oos_confusion[, 1]), "\n")
cat("miscl err =", (oos_confusion[2, 1] + oos_confusion[1, 2]) / length(y_test), "\n")
```


Another thing to consider: did we need phats? Do we trust the default rule, yhat = indicator(phat > 0.5)??

We really only need those if we care about proabilities or wish to create an asymmetric cost classifier. Why not optimize for hinge loss instead here? Looks like overfitting?

Here it seems default RF beats default xgboost on both loss functions. So we need to CV the hyperparams for xgboost. In the era of big data, it's not a problem to have a huge Dselect.

And on letters:

```{r}
rm(list = ls())
boosting_hyperparams = list(
 eta = 0.3, #default step size
 max_depth = 6, #default 
 nrounds = 3000, #M, we raise it here for classification
 subsample = 1, #default 
 colsample_bytree = 1, #default
 nthread = 11,
 print_every_n = 100
)

pacman::p_load(mlbench)
data(LetterRecognition)
letter = na.omit(LetterRecognition) #kill any observations with missingness
letter_mm = model.matrix(lettr ~ ., letter)

set.seed(1)
train_size = 2000 #18000
train_indices = sample(1 : nrow(letter), train_size)
letter_train = letter[train_indices, ]
y_train = as.numeric(letter_train$lettr) - 1
X_train = letter[train_indices, -1]
X_train_mm = letter_mm[train_indices, ]

test_size = 2000
test_indices = sample(setdiff(1 : nrow(letter), train_indices), test_size)
letter_test = letter[test_indices, ]
y_test = letter_test$lettr
X_test_mm = letter_mm[test_indices, ]

xgboost_mod = xgboost(
 data = X_train_mm,
 label = y_train,
 objective = "multi:softmax", #multinomial classification setting
 num_class = 26,
 #######hyperparameters
 nrounds = 3000,
 eta = boosting_hyperparams$eta,
 max_depth = boosting_hyperparams$max_depth,
 subsample = boosting_hyperparams$subsample,
 colsample_bytree = boosting_hyperparams$colsample_bytree,
 nthread = boosting_hyperparams$nthread,
 print_every_n = boosting_hyperparams$print_every_n
)

y_hat_test = predict(xgboost_mod, X_test_mm)
oos_confusion = table(y_test, LETTERS[y_hat_test + 1])
cat("default boosting")
cat("miscl err =", (sum(oos_confusion) - sum(diag(oos_confusion))) / length(y_test), "\n")

rf_mod = YARF(X_train, as.factor(y_train))

y_hat_test = predict(rf_mod, letter_test)
oos_confusion = table(y_test, y_hat_test)
cat("default RF")
cat("miscl err =", (sum(oos_confusion) - sum(diag(oos_confusion))) / length(y_test), "\n")
```

We can try playing with the hyperparams. A lot to CV over! You think this is a lot of hyperparameters... wait until you see deep learning networks!

But where xgboost really shines is it's fast on large datasets where RF can't even work at all. So even if it doesn't win in predictive performance, it wins in practicality.


A few more interesting things:


```{r}
xgb.plot.tree(model = xgboost_mod, trees = 0)
```

That's a plot of one of the trees (not so useful as the model is a sum of 1000s of trees)

```{r}
xgb.ggplot.deepness(xgboost_mod)
```

This shows you how deep the tree models are.

```{r}
importance_matrix = xgb.importance(colnames(X_train_mm), model = xgboost_mod)
pacman::p_load(Ckmeans.1d.dp)
xgboost::xgb.ggplot.importance(importance_matrix)
```

The feature importance metrics comes from how much the attribute split points improve performance weighted by the number of observations in the node.

RF has a similar feature and there are many ways to define "importance" within RF:

```{r}
pacman::p_load(forcats)
var_counts = query_variable_counts(rf_mod)
ggplot(data.frame(variable = forcats::fct_reorder(names(var_counts), var_counts, .desc = TRUE), count = var_counts)) + 
  geom_col(aes(x = variable, y = count))
```

It's a different model - so different importances!

You should also try the following packages: catboost and lightgbm.

I think the point of this demo is it's difficult to get an optimal model - there's lots of choices. Thus machine learning is part art and part science. I think that's a good final takeaway message for this whole course!


