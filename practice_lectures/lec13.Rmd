---
title: "Practice Lecture 13 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


# Logistic Regression for Binary Response

Let's clean up and load the cancer dataset, remove missing data, remove the ID column and add more appropriate feature names:

```{r}
biopsy = MASS::biopsy
biopsy$ID = NULL
biopsy = na.omit(biopsy)
colnames(biopsy) = c( #should've named them appropriately a few lectures ago
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
head(biopsy$class)
```

We can either estimate probability of the biopsy tissue being benign (this would mean y = 1 is the benign category level) or estimate the probability of the biopsy tissue being malignant (this would mean y = 1 is the malignant category level).

Let's go with the latter. To make the encoding explicitly 0/1, we can cast the factor to numeric or we can rely on R's default factor representation i.e. that the first level is 0 and the second level is 1. Here, we can use this default without reordering since the levels above show that benign is first and thus = 0 and malignant is second and thus = 1 (via coincidence of alphabetical order).

Now let's split into training and test for experiments:

```{r}
set.seed(1984)
K = 5
test_prop = 1 / K
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a linear logistic regression model. We use the function `glm` which looks a lot like `lm` except we have to set the family parameter to be "binomial" which means we are using the independent Bernoulli and within the binomial family, we are using the "logit" link. There are other types of family models we won't get a chance to study e.g. Poisson, negative binomial for count models

```{r}
logistic_mod = glm(class ~ ., biopsy_train, family = binomial(link = "logit"))
```

That was fast! There was actually a lot of optimization in that line. Let's look at the $b$ vector that was made:

```{r}
coef(logistic_mod)
```

Interpretation? If clump thickness increases by one unit the log odds of malignancy increases by 0.597...

All of the coefficients are positive which means if any of the covariates increase...

And let's take a look at the fitted values:

```{r}
head(predict(logistic_mod, biopsy_train))
```

What's that? Those are the "inverse link" values. In this case, they are log-odds of being malignant. If you can read log odds, you'll see subject #... has a small probability of being malignant and subject #... has a high probability of being malignant. It's not that hard to read log odds...

What if we want probabilities? We can tell the predict function for `glm` to give us them explicitly:

```{r}
head(predict(logistic_mod, biopsy_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

We see lots of phats close to zero and lots close to one. The model seems very sure of itself! 

Let's see response by estimated probability another way using a box and whisker plot:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Made only a few mistakes here and there in the training set! How about the test set?

```{r}
p_hats_test = predict(logistic_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = y_test), alpha = 0.5)
ggplot(data.frame(p_hats_test = p_hats_test, y_test = factor(y_test))) + 
  geom_boxplot(aes(x = y_test, y = p_hats_test))
```

Looks pretty good! 

We now will talk about error metrics for probabilistic estimation models. That will give us a way to validate this model and provide an estimate of future performance. 

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Yup can't do arithmetic operations on factors. So now we have to go ahead and cast.

```{r}
y_train_binary = ifelse(y_train == "malignant", 1, 0)
mean(-(y_train_binary - p_hats_train)^2)
```

This is very good Brier score! Again, most of the probabilities were spot on. And the oos Brier score?

```{r}
y_test_binary = ifelse(y_test == "malignant", 1, 0)
mean(-(y_test_binary - p_hats_test)^2)
```

Not as good but still very good!

What is the in-sample log score?

```{r}
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
```

This isn't bad (if you get intuition in reading them). And oos?

```{r}
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Not as good but still very good!

If we wanted to be more careful, we can use K-fold CV to get a less variable oos metric. Maybe we'll do that in a lab?


# Probit and Cloglog probability estimation

These are different generalized linear models but fit using the same code. All we need to do is change the link argument. For a probit regression we just do:

```{r}
probit_mod = glm(class ~ ., biopsy_train, family = binomial(link = "probit"))
```

This is complaining about numerical underflow or overflow. If you get a z-score that's really large in magnitude, then it says probability is 1 (if z score is positive) or 0 (if z score is negative)

```{r}
coef(probit_mod)
```

As we saw before, all coefficients for the covariates are positive. What's the interpretation of b for bare_nuclei?

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(probit_mod, biopsy_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = y_train), alpha = 0.5)
```

This is basically the same. How about out of sample?


```{r}
p_hats_test = predict(probit_mod, biopsy_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Also basically the same-looking. To get an apples-apples comparison with logistic regression let's calculate the brier and log scoring metrics:

```{r}
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

It appears the logistic regression is slightly better than probit regression oos.

Let's do complementary log-log too:

```{r}
cloglog_mod = glm(class ~ ., biopsy_train, family = binomial(link = "cloglog"))
coef(cloglog_mod)
```

Same signs on coefficients. Interpretation? Difficult... 

Let's see how it does compared to the logistic and probit models.

```{r}
p_hats_train = predict(cloglog_mod, biopsy_train, type = "response")
p_hats_test = predict(cloglog_mod, biopsy_test, type = "response")
mean(-(y_train_binary - p_hats_train)^2)
mean(-(y_test_binary - p_hats_test)^2)
mean(y_train_binary * log(p_hats_train) + (1 - y_train_binary) * log(1 - p_hats_train))
mean(y_test_binary * log(p_hats_test) + (1 - y_test_binary) * log(1 - p_hats_test))
```

Much worse than either! 

Logistic regression is usually the default. But just because it's the default and most popular and just because it won here doesn't mean it will always win!! Using probit or any other link function constitutes a completely different model. You can use the "model selection procedure" we will discuss to choose which link function is best for your dataset.

Let's try a harder project... load up the adult dataset where the response is 1 if the person makes more than \$50K per year and 0 if they make less than \$50K per year in 1994 dollars. That's the equivalent of almost $90K/yr today (see https://www.in2013dollars.com/us/inflation/1994?amount=50000).

We must load this package from github repository as it's not on CRAN.

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
?adult
#to make the exercise easier I'm going to delete two features - it's more work to get oos validation working if these two stay in which I can explain later
adult$occupation = NULL
adult$native_country = NULL
skimr::skim(adult)
```

What is g_0?

```{r}
mean(as.numeric(adult$income == ">50K"))
```

Let's use samples of 5,000 to run experiments:

```{r}
set.seed(1984)

train_size = 5000
train_indices = sample(1 : nrow(adult), train_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL

test_size = 5000
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Let's fit a logistic regression model to the training data:

```{r}
logistic_mod = glm(income ~ ., adult_train, family = "binomial") #shortcut for binomial(link = "logit")
```

Numeric errors already!

Let's see what the model looks like:

```{r}
coef(logistic_mod)
length(coef(logistic_mod))
```

There may be NA's above due to numeric errors. Usually happens if there is linear dependence (or near linear dependence). Interpretation?

Let's take a look at the fitted probability estimates:

```{r}
head(predict(logistic_mod, adult_train, type = "response"))
```

Let's take a look at all the in-sample probability estimates:

```{r}
p_hats_train = predict(logistic_mod, adult_train, type = "response")
pacman::p_load(ggplot2)
ggplot(data.frame(p_hats_train = p_hats_train, y_train = y_train)) + 
  geom_histogram(aes(x = p_hats_train, fill = factor(y_train)), alpha = 0.5)
```

Much more humble than the cancer data's model!! It's not a very confident model since this task is much harder! In fact it's never confident about the large incomes and only confident about the small incomes half the time. If not confident, it is predicting probs away from 0% and 100%.

Let's see $y$ by $\hat{p}$:

```{r}
ggplot(data.frame(p_hats_train = p_hats_train, y_train = factor(y_train))) + 
  geom_boxplot(aes(x = y_train, y = p_hats_train))
```

Making lots of mistakes!

Note that the x-axis is the native category label since we never coded as 0, 1. The default is that the first label is 0 and the second is 1. The labels are defaulted to alphabetical order (I think...)

What is the in-sample average Brier score?

```{r}
mean(-(y_train - p_hats_train)^2)
```

Can't use factors here. Need to code the response as 0/1

```{r}
y_train_numeric = ifelse(y_train == ">50K", 1, 0)
mean(-(y_train_numeric - p_hats_train)^2)
```

This is worse than the previous dataset but not terrible. The null model gives what?

```{r}
mean(-(y_train_numeric - rep(mean(y_train_numeric), length(y_train_numeric)))^2)
```

So this is indeed a major improvement.

But this was in sample! Let's see what happens out of sample..


```{r}
p_hats_test = predict(logistic_mod, adult_test, type = "response")
ggplot(data.frame(p_hats_test = p_hats_test, y_test = y_test)) + 
  geom_histogram(aes(x = p_hats_test, fill = factor(y_test)), alpha = 0.5)
```

Looks similar to training. And the Brier score?

```{r}
y_test_numeric = as.numeric(y_test) - 1
mean(-(y_test_numeric - p_hats_test)^2)
```

The oos performance is about the same as the in-sample performance so we probably didn't overfit. This makes sense, we had $n = 5000$ and $p + 1 = 13$.

Brier scores only make sense if you know how to read Brier scores. It's kind of like learning a new language. However, everyone understands classification errors! We will see these performance metrics later in the semester.


# "Nonlinear Linear" Regression with polynomials

Even though we can demonstrate "Nonlinear Linear" Regression with polynomials using glm's for probability estimation, it is easier when the response is numeric.

Let's generate a polynomial model of degree 2 ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

```{r}
set.seed(1003)
n = 25
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n, -2, 5)
#best possible model
h_star_x = beta_0 + beta_1 * x + beta_2 * x^2

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
pacman::p_load(ggplot2)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
head(x)
```

Let's try to estimate with a line:

```{r}
linear_mod = lm(y ~ x)
b_linear = summary(linear_mod)$coef
basic + geom_abline(intercept = b_linear[1], slope = b_linear[2], col = "red")
```

The relationship is "underfit". $\mathcal{H}$ is not rich enough right now to express something close to $f(x)$. But it is better than the null model!

Now let's do a polynomial regression of degree two. Let's do so manually:

```{r}
X = as.matrix(cbind(1, x, x^2))
head(X)
b = solve(t(X) %*% X) %*% t(X) %*% y
b
c(beta_0, beta_1, beta_2)
```

These are about the same as the $\beta_0, \beta_1$ and $\beta_2$ as defined in $f(x)$ the true model. In order to graph this, we can no longer use the routine `geom_abline`, we need to use `stat_function`.

```{r}
plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}

basic + 
  stat_function(fun = plot_function_degree_2, args = list(b = b), col= "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2)), col= "darkgreen")
```

Now let's try polynomial of degree 3:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_3 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3
}

basic + stat_function(fun = plot_function_degree_3, args = list(b = b), col = "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2, 0)), col= "darkgreen")
```

Still the same. Why? The $x^3$ term is like adding one "nonsense" predictor. One nonsense predictor marginally affects $R^2$ but it doesn't matter too much.

Now let's try polynomial of degree 8:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_8 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 
}

basic + stat_function(fun = plot_function_degree_8, args = list(b = b), col = "red") + 
  stat_function(fun = plot_function_degree_2, args = list(b = c(beta_0, beta_1, beta_2, rep(0, 6))), col= "darkgreen")
```

We are seeing now a little bit of "overfitting" in the edge(s). We now have $p=9$ and $n=100$. We can do a lot worse!

Let's learn how to do this in R first without having to resort to manual linear algebra. R has a function called "poly" that can be used *inside* formula declarations.

Let's first fit the degree 2 model:

```{r}
degree_2_poly_mod = lm(y ~ poly(x, 2, raw = TRUE))
head(model.matrix(~ poly(x, 2, raw = TRUE))) #the model matrix for this regression - just to check
b_poly_2 = coef(degree_2_poly_mod)
b_poly_2
summary(degree_2_poly_mod)$r.squared
```

Let's go on a slight tangent. And look at this regression without using the raw polynomial.

```{r}
Xmm = model.matrix(~ poly(x, 2))
head(Xmm) #the model matrix for this regression - just to check
Xmm[, 1] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 3]
Xmm[, 3] %*% Xmm[, 3]
```

Are these orthogonal polynomials? How is the `poly` function without `raw = TRUE` working to generate a model matrix?

```{r}
degree_2_orthog_poly_mod = lm(y ~ poly(x, 2))
b_poly_2 = coef(degree_2_orthog_poly_mod)
b_poly_2
summary(degree_2_orthog_poly_mod)$r.squared
```

Raw or orthogonal does not affect the yhats and Rsq. They are the same as we got before! That's because the colspace is the same in both cases raw or polynomial. We use "raw" polynomials to keep them interpretable and on the same scale as the manual models we were fitting.

Now let's do polynomial of degree 13:

```{r}
degree_13_poly_mod = lm(y ~ poly(x, 13, raw = TRUE))
b_poly_13 = coef(degree_13_poly_mod)

plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

basic + stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")# + ylim(c(0, 25)) #+ xlim(c(-2, 5.2))
```

What's happening for small values of $x$ (and a bit for large values)? This is called [Runge's Phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) meaning that the boundary activity of high-order polynomials has very large derivatives. Let's go back to the same scale as before and see what's happening:

```{r}
basic + 
  coord_cartesian(xlim = c(-2, 5), ylim = c(-3, 25)) + 
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")
```

This is terrible! Future predictions will be awful at the edges and even more awful when extrapolating beyond the edges! Let's examine this extrapolation further using another dataset.


# Orthogonal vs raw polynomials

Why is orthonormal polynomial the default? You can argue that doing a QR decomposition on the polynomial expansion and employing Q in the design matrix will change b thereby making b uninterpretable! So why use orthonormal polynomials? Here's why:

```{r}
n = 1000
set.seed(1984)
X = data.frame(x = c(runif(n / 2, 0, 1e-2), runif(n / 2, 0, 1e6)))
d = 10
num_digits = 8
Xmm_orth = model.matrix(~ 0 + poly(x, d), X)
colnames(Xmm_orth)[1 : d] = 1 : d
Xmm_raw = model.matrix(~ 0 + poly(x, d, raw = TRUE), X)
colnames(Xmm_raw)[1 : d] = 1 : d
```

Let's look at the design matrix for small values of x:

```{r}
head(as.matrix(X))
round(head(Xmm_orth), num_digits)
round(head(Xmm_raw), num_digits)
```

You get numerical underflow almost immediately when using the raw polynomial computations (you get it by degree 4). And thus you can't even get the OLS estimates:


```{r}
y = rnorm(n)
solve(t(Xmm_raw) %*% Xmm_raw) %*% t(Xmm_raw) %*% y
```

Let's look at the design matrix for large values of x:

```{r}
tail(as.matrix(X))
round(tail(Xmm_orth), num_digits)
round(tail(Xmm_raw), num_digits)
```

You get numerical overflow in the design matrix (but it will happen later). But the second you start to use the design matrix with 10^59's inside...

```{r}
solve(t(Xmm_raw[800 : 1000, ]) %*% Xmm_raw[800 : 1000, ]) %*% t(Xmm_raw[800 : 1000, ]) %*% y
```

As opposed to

```{r}
solve(t(Xmm_orth) %*% Xmm_orth) %*% t(Xmm_orth) %*% y
```

No problem at all!!!

So that's the reason: numerical stability. But if you need interpretability, you need raw polynomials. But if you're interpreting the model, how do you even interpret beyond degree 2???


#Extrapolation vs Interpolation

Let's take a look at the Galton Data again.

```{r}
pacman::p_load(HistData, ggplot2)
data(Galton)
mod = lm(child ~ parent, Galton)
b_0 = mod$coefficients[1]
b_1 = mod$coefficients[2]
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue", size = 1) +
  xlim(63.5, 72.5) + 
  ylim(63.5, 72.5) +
  coord_equal(ratio = 1)
```

Let's say I want to predict child's height for parents' average height of 70in. All I do is:

```{r}
predict(mod, data.frame(parent = 70))
```

What if I want to predict for a parents' height of 5in. Is there any 12in tall human being? No... it is absurd. But nothing stops you from doing:

```{r}
predict(mod, data.frame(parent = 5))
```

That's [actually possible](https://www.guinnessworldrecords.com/news/2012/2/shortest-man-world-record-its-official!-chandra-bahadur-dangi-is-smallest-adult-of-all-time/).

Look at our linear model from Euclid's perspective:

```{r}
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue", size = 1) +
  xlim(-20, 120) + 
  ylim(-20, 120) +
  coord_equal(ratio = 1)
```

What is a linear model with $p + 1 = 2$. It's just a line. When geometry was first formalized by Euclid in the Elements, he defined a line to have "breadthless length" with a straight line being a line "which lies evenly with the points on itself". By "breadthless" he meant infinite in either direction. There is no mathematical problem with predicting childrens' heights using negative parents' heights e.g.

```{r}
predict(mod, data.frame(parent = -5))
```

But this is absurd. So now we need to talk about a fundamental concept in data science we've been kind of avoiding and one that most people ignore. There are two types of prediction: interpolation and extrapolation. Interpolation is essentially the type of "prediction" we've been talking about this whole class. 

Extrapolation is totally different. It's what happens when you predict outside of the range of the covariate data you've seen in $\mathbb{D}$. Extrapolation is very dangerous - your models only work based on $\mathbb{D}$. Extrapolation is prediction outside of the range you've seen before which means. You better have a good theoretical reason as to why your $\mathbb{H}$ function class will extend outside that range. Because each $\mathbb{H}$ function class will extrapolate very very differently.



What happens during extrapolation? Let's look at the (a) linear model, (b) polynomial model with degree 2 and (c) polynomial with degree 13.

```{r}
degree_2_poly_mod = lm(child ~ poly(parent, 2, raw = TRUE), Galton)
b_poly_2 = coef(degree_2_poly_mod)
degree_13_poly_mod = lm(child ~ poly(parent, 13, raw = TRUE), Galton)
b_poly_13 = coef(degree_13_poly_mod)
b_poly_13[is.na(b_poly_13)] = 0

plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}
plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

# xymin = 65
# xymax = 71
xymin = 50
xymax = 90
ggplot(Galton, aes(x = parent, y = child)) + 
  geom_point() + 
  geom_jitter() +
  geom_abline(intercept = b_0, slope = b_1, color = "blue") +
  coord_cartesian(xlim = c(xymin, xymax), ylim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_2, args = list(b = b_poly_2), col = "red", xlim = c(xymin, xymax)) +
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "orange", xlim = c(xymin, xymax))
```

Besides being bad at the edges of the input space, polynomial models have *TERRIBLE* performance when you leave input space. The extrapolation risk is totally unpredictable due to Runge's phenomenon. Please do not use them if you think you'll ever extrapolate!!!

# Overfitting with Polynomials

Can we achieve $R^2 = 100\%$ using polynomial regression? Yes. Here's an example in one dimension. These are called "interpolation polynomials". In one dimension, as long as the $x$ values are distinct, $n$ data point can be fit by a $n - 1$ degree polynomial. Here's an example with a few data points:

```{r}
set.seed(1003)
n = 5
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n)

h_star_x = beta_0 + beta_1 * x + beta_2 * x^2
y = h_star_x + rnorm(n)
#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
```

Now fit polynomial models:

```{r}
degree_4_poly_mod = lm(y ~ poly(x, 5, raw = TRUE))
b_poly_4 = coef(degree_4_poly_mod)

plot_function_degree_4 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4
}

basic + stat_function(fun = plot_function_degree_4, args = list(b = b_poly_4), col = "purple")
```

Perfect fit!

```{r}
summary(degree_4_poly_mod)$r.squared
```

This is the same thing we've seen before! If $n = p + 1$, then the design matrix is square and there is no need to project onto a lower dimensional subspace. To estimate the linear model, one only needs to solve $n$ equations with $n$ unknowns.

My recommendations:
1) Keep polynomial degree low. Preferably 2. Anything past 2 is not interpretable anyway. We didn't talk about "interpretability" of models yet, but you get the idea. If you are using degree = 2, use raw polynomials so you get interpretability (see next section).
2) Be very careful not to extrapolate: make sure future predictions have the measurements within range of the training data $\mathbb{D}$. Extrapolations are going to be very, very inaccurate. Polynomial regressions I'm sure have gotten data scientists fired before.



## Model Selection and Three Data Splits

This unit is split into three use cases (1) Selection among M explicit models (2) Hyperparameter Selection within one algorithm (3) Stepwise Model Construction

# Use Case (I) Selecting one of M Explicit Models

We have now covered non-linearities (e.g. polynomial terms) and interactions. A new complication now clearly emerges. If I have $p$ predictors, there are many linear least squares models I can build (considering non-linear least squares models makes the space of models even larger!!)

For instance, here are a bunch of models and their in-sample performance:

```{r}
pacman::p_load(ggplot2)
all_model_formulas = list( #note: need these as strings for later...
  "price ~ carat + depth",
  "price ~ carat + depth + color + x + y + z",
  "price ~ .",
  "price ~ . * ."
)
mods = lapply(all_model_formulas, lm, diamonds)
lapply(mods, function(mod){summary(mod)$sigma})
```

Obviously the in-sample RMSE's are increasing due to the complexity, but which model is "best"?

This is one of the most fundamental problems in statistics, and possibly all of science! 

In class, we discussed validation via dividing $\mathbb{D}$ into (a) a training set and a (b) testing set. Now, we will further divide the training set into (a) a sub-training set and a (b) selection set and we still have the (c) test set. 

The total training set together will fit a model and testing will estimate future performance. But within the total training set, we'll use an elaborate algorithim: we'll fit many models and take the best one. That's the "master algorithm".

We'll make the selection set and the test set the same size but we don't have to. First split up the data:

```{r}
n = nrow(diamonds)
K = 5
set.seed(1984)
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

#make sure we did this right:
pacman::p_load(testthat)
expect_equal(1 : n, sort(c(train_indices, select_indices, test_indices)))

diamonds_train = diamonds[train_indices, ]
diamonds_select = diamonds[select_indices, ]
diamonds_test = diamonds[test_indices, ]

rm(test_indices, select_indices, train_indices)
```

Now, fit all models and select the best one:

```{r}
mods = lapply(all_model_formulas, lm, diamonds_train)
```

Now predict on the selection set and look at the oos $s_e$, and select the "best" model

```{r}
yhat_selects = lapply(mods, function(mod){predict(mod, diamonds_select)})
y_select = diamonds_select$price #the true prices

s_e_s = lapply(yhat_selects, function(yhat_select){sd(yhat_select - y_select)})
s_e_s
#find the minimum
which.min(s_e_s)
```

Which are overfit? Which are underfit? Were these models "poor choices"?

Can we go back and fit some more models? 

Yes - as long as we don't open the "lockbox" of the test set. Let's look at one more model. An expansion of the best of the previous 4 models now with a couple interactions we are convinced are real plus a couple of non-linear terms:

```{r}
all_model_formulas[[5]] = "price ~ . + carat * color + carat * depth + I(carat^2) + I(depth^2)"
mods[[5]] = lm(all_model_formulas[[5]], diamonds_train) 

yhat_selects[[5]] = predict(mods[[5]], diamonds_select)

s_e_s[[5]] = sd(yhat_selects[[5]] - y_select)
s_e_s
#find the minimum
which.min(s_e_s)
```

We can go further and fit more and more models but we should always be careful that we don't fit too many as we may optimize to the selection set. Here, we are lucky since the selection set is large (~11,000 observations) so this is not too much of a fear.

But you can see the problem - how can we build a good model??

The answer to this is non-parametric regression / machine learning. But first, we will cover two other important topics before we get there.

Let us return and complete the exercise by now declaring we are done modeling and we are going to ship model 5. Let us get a conservative estimate of its performance:

```{r}
mod5_for_test = lm(all_model_formulas[[5]], rbind(diamonds_train, diamonds_select))
yhat_test_mod5 = predict(mod5_for_test, diamonds_test)
y_test = diamonds_test$price #the true prices
sd(yhat_test_mod5 - y_test)
```

About the same as the selection estimate --- we did not overfit too much to the selection set.

At this point the lockbox is open and we can never return (if we are honest, of course - many people in this business are not but it's never worth it).

Now we can build production model 4 with all data to ship:

```{r}
mod_final = lm(all_model_formulas[[5]], diamonds)
```

No evaluation is done on the final model. It is only used to predict future diamonds' prices.

We can make two improvements using CV to the above model selection procedure:

* To reduce variance in the selection process, you make a CV of the selection set. 
* To reduce variance in the testing process, you make an outer CV of the test set so that the first CV is a nested resampling. 

This is a lot more coding! But we're in luck because it's done for us already. I will eventually demo the `mlr3` package which makes this easy because it's too difficult to code from scratch without bugs.


# Use Case (II) Hyperparameter Selection

Remember the `lambda` from the Vapnik Objective Function which we minimized to compute an SVM for non-linearly separable data? We now have a way to automatically select its value. Each lambda value implies a different model. Hence we need to do "model selection" to select a hyperparameter which selects our model with the best performance.

Let's demo this on the `adult` dataset. Due to computational concerns only, let's limit the dataset to be size n = 3000 with 1/3-1/3-1/3 training-select-test sets.

```{r}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
n = 3000
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
```

We now load the SVM package. Recall how to use the SVM: you specify a "cost" argument. That cost is related to the lambda value.

```{r}
pacman::p_load(e1071)
#e.g. 
#svm(income ~ ., adult_train, kernel = "linear", cost = 0.1)
```

So now we need to pick a bunch of cost values to search over. This is called a "grid search". It's still a decision! How many values in the grid? Step size? Minimum? Maximum? 

Let's search over 50 models. And we can specify cost values on the log10 scale. Let minimum be 1e-6 and maximum be 100. 

```{r}
M = 50
cost_grid = 10^seq(from = -6, to = 2, length.out = M)
cost_grid
```

Let's do our search and collect errors on the select set:

```{r}
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
  #train on train set
  svm_mod = svm(income ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
  #predict on select set
  y_hat = predict(svm_mod, adult_select)
  #measure error from the select set
  select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
```

Let's plot these oos misclassification error rates:

```{r}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) + 
  aes(x = cost, y = miscl_err) +
  geom_line(color = "grey") + 
  geom_point(lwd = 3) + 
  scale_x_log10()
```

Looks like the grid gave us a natural minimum. We can find the best value of the cost hyperparameter and its associated misclassification error:

```{r}
min(select_set_misclassification_errors_by_m)
optimal_cost_hyperparam = cost_grid[which.min(select_set_misclassification_errors_by_m)]
optimal_cost_hyperparam
```

Assume we will use this model and not test any more values of cost, we can get an honest performance metric for this optimally-tuned hyperparameter svm for the future on the test set:

```{r}
svm_mod = svm(income ~ ., adult_train, kernel = "linear", cost = optimal_cost_hyperparam)
y_hat = predict(svm_mod, adult_test)
mean(adult_test$income != y_hat)
```

This is about the same as we found on the select set. Thus, we didn't overfit the select set too much.

To ship the final model, use all the data.

```{r}
g_final = svm(income ~ ., adult, kernel = "linear", cost = optimal_cost_hyperparam)
```


# Use Case (III) Forward Stepwise Model Construction

There are many types of such stepwise models. Here we will look at Forward Stepwise Linear models. "Forward" meaning we start with a low complexity model and end with a high complexity model, "Stepwise" meaning we do so iteratively which each step consisting of one additional degree of freedom i.e. one incremental increase in complexity and "Linear" meaning that the model is linear. By default we use OLS.

We will be using the diamonds data again as an example. Let's make sure we have unordered factors to avoid issues later:

```{r}
rm(list = ls())
diamonds = ggplot2::diamonds
diamonds$cut = factor(diamonds$cut, ordered = FALSE)
diamonds$color = factor(diamonds$color, ordered = FALSE)
diamonds$clarity = factor(diamonds$clarity, ordered = FALSE)
```

What we're doing will be highly computational, so let's take a random sample of the diamonds in $\mathbb{D}$ for training, selecting and testing:

```{r}
Nsamp = 1300
set.seed(1984)
subindices = sample(1 : nrow(diamonds), Nsamp * 3)
diamonds_train = diamonds[subindices[1 : Nsamp], ]
diamonds_select = diamonds[subindices[(Nsamp + 1) : (2 * Nsamp)], ]
diamonds_test = diamonds[subindices[(2 * Nsamp + 1) : (3 * Nsamp)], ]
rm(subindices)
```

Let's built a model with all second-order interactions e.g. all things that look like depth x table x clarity or depth^2 x color or depth^3.

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
```

Here's what some of the feature names look like:

```{r}
sample(names(coef(mod)), 100)
```

For features that are non-binary, the total will be p_non_binary^3 features. Binary features are more complicated because its each level in feature A times each level in feature B. There are no squared or cube terms for binary features (since they're all the same i.e. ${0,1}^d = {0,1}$).

Remember we likely overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions.

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try predicting using this insano model on another Nsamp we didn't see...

```{r}
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ and awful RMSE --- why? We overfit big time!

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck! This means you can do ~60x better using the null model (average of y) instead of this model.

So let us employ stepwise to get a "good" model. We need our basis predictors to start with. How about the linear components of `. * . * .` --- there's nothing intrinsically wrong with that - it's probably a good basis for $f(x)$. Let's create the model matrix for both train and test:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)
Xmm_select = model.matrix(price ~ . * . * ., diamonds_select)
y_select = diamonds_select$price
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r}
included_features_by_iter = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (j_try %in% included_features_by_iter){
      next 
    }
    Xmm_sub = Xmm_train[, c(included_features_by_iter, j_try), drop = FALSE]
    all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
  }
  j_star = which.min(all_ses)
  included_features_by_iter = c(included_features_by_iter, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, included_features_by_iter, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_select = Xmm_select[, included_features_by_iter, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_select - y_hat_select)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i =", i, "in sample: se = ", round(all_ses[j_star], 1), "oos_se", round(oos_se, 1), "added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
}
```

Now let's look at our complexity curve:

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylim(0, max(c(simulation_results$in_sample_ses_by_iteration, simulation_results$oos_ses_by_iteration)))
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. There is a whole zone of flat models. Better to be simple. So let's pick it from the graph visually. You can also obviously do CV within each iterations to stabilize this further.

What's the optimal number of features? And its associated oos s_e?

```{r}
p_opt = 40 #change this based on plot above
oos_ses_by_iteration[p_opt]
```

What are those features?

```{r}
sort(colnames(Xmm_train)[included_features_by_iter[1 : p_opt]])
```

Lots of carat-color-clarity interactions which makes sense. Other features don't matter so much as their information is probably duplicated within carat, color and clarity.

What is the "true optimal model"? It's impossible to find as we'd have to search all subsets (of which there are exponential number of them 2^1000 = 10^31). Here we used a greedy search so hopefully we get somewhere in the ballpark of optimal. 

We now get a conservative estimate of its future performance:

```{r}
Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
optimal_mod = lm(diamonds_test$price ~ 0 + Xmm_test[, included_features_by_iter[1 : p_opt]])
summary(optimal_mod)$sigma
```

Looks even better than we found during the stepwise iterations on the select set. This is random variation. It would be better to use CV within the stepwise and CV outside to then stabilize this final estimate.

The final step is to fit g_final on all data using those selected features.


