---
title: "Practice Lecture 16 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---



# Log transformations

We will be examining the diamonds dataset. Let's take a moment to read about our data. In real life, you will take more than just a moment to learn about the data. And more than just a moment to clean the data and do sanity checks. As these steps are more of the grunt work of data science (and are learned on the fly), I won't really cover them formally.

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal
diamonds = diamonds[sample(1 : nrow(diamonds)), ] #shuffle for better visualizations
dim(diamonds)
```

That's a huge $n$. So, let's expect things to take a bit longer when processing.

A natural increasing relationship will likely be found between weight and price. Let's see it visually:

```{r}
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point()
```


How good does a best guess linear relationship do?

```{r}
mod = lm(price ~ carat, diamonds)
b = coef(mod)
b
summary(mod)$r.squared
summary(mod)$sigma
```

What does the intercept say about extrapolation?

Let's see the best fit line $g(x)$ visually:

```{r}
ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green") + ylim(0, 50000)
```

Maybe the relationship between weight and price is not linear - but exponential? E.g. if the weight of a diamond doubles, maybe the price quadruples? Or linear increases in weight yield percentage increases in price. This seems plausible. 

Besides the plausibility of the functional form, there is an agnostic reason to employ log y as the predictive target. Let's first examine the univariate data!


```{r}
skimr::skim(diamonds$price)
```

Very large standard error and very long tail which can be seed more clearly here:

```{r}
ggplot(diamonds) + geom_histogram(aes(price), binwidth = 200)
```

Let's take a look at the distribution after logging:

```{r}
ggplot(diamonds) + geom_histogram(aes(x = log(price)), binwidth = 0.01)
```

Some strange artifacts appear. Why the gap? Why is it "cut" sharply at a maximum? These are questions to ask the one who collected the data. But let's get back to the log story...

Popular wisdom says logging this type of highly skewed-right distribution would possibly make the model "more linear in x". Put another way, it would be easier to "catch" (predict) the long tail since it won't be a long tail anymore after you log-transform. It would also prevent observations with large y's becoming "leverage points" i.e. points that unduly influence the model and thereby warp its ability to predict the average observation. In 343, you'll learn more reasons for why you should use log the response, but those reasons are relevant for inference so we won't discuss them here.

Let's give the model with ln(y) a whirl. Maybe we'll even learn something about diamonds. The way to create such a model is to simply fit an OLS model to log y. This is called a log-linear model. Since this is a pretty standard thing to do so R's formula notation has it built-in as follows:

```{r}
log_linear_mod = lm(log(price) ~ carat, diamonds)
b = coef(log_linear_mod)
b
```

Let's see what this looks like.

```{r}
ggplot(diamonds, aes(x = carat, y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green") + ylim(6, 12)
```

It looks very bad if carat is large. That means our little theory about carats getting larger yielding multiples of price doesn't correspond to reality.

How did we do?

```{r}
summary(log_linear_mod)$r.squared
summary(log_linear_mod)$sigma
```

Look at that RMSE! That dropped like a rock! Is that real?

No. RMSE before is in the units of y. And now y is now in ln($). So this RMSE and the previous RMSE are *not* comparable.

The $R^2$ are *not* comparable either. Even though they're on a [0, 1] scale in both models, the SST's are different so you're measuring the proportion of a different variance.

Let's attempt to compare apples-apples.

```{r}
log_y_hat = log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

What does this mean? It means this was a bad idea. Those residuals for large carats are insanely large. They're wrong on a log scale! Which means they're off by orders of magnitude. Working with logged y is dangerous business if you're wrong! Before you were off by a few thousand dollars; now you're off by millions. For example. Let's look at a large diamond:

```{r}
xstar = diamonds[diamonds$carat > 5, ][1, ]
xstar$price
predict(mod, xstar)
exp(predict(log_linear_mod, xstar))
```

That's a pretty bad residual!

How about log-log model? 

```{r}
log_log_linear_mod = lm(log(price) ~ log(carat), diamonds)
b = coef(log_log_linear_mod)
b
```

Let's see what it looks like:

```{r}
ggplot(diamonds, aes(x = log(carat), y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

Well look at that! That's a nice looking model. (Note that the slope coefficients in log-log models, i.e. b_2 here, are called "elasticity" in Econ 382 as it measures how the relative change in x affects the relative change in y).

How are our metrics?

```{r}
summary(log_log_linear_mod)$r.squared
summary(log_log_linear_mod)$sigma
```

Let's see apples-to-apples to the natural y model.

```{r}
log_y_hat = log_log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

This is on-par with the vanilla OLS model, but still doesn't "beat it". There was no guarantee that we would be "beat it" even though we used procedures that are reasonable and popular!

Let's repeat this entire exercise using the length of the diamond. The length of the diamond feature is confusingly named "x" in the dataset. It is an "x" but it's also the diamond's "x"!!!

```{r}
ggplot(diamonds, aes(x = x, y = price)) + 
  geom_point()
```

Besides the non-linear relationship, what else do you see? Mistakes in the dataset! Can a real diamond have zero length?? Yes. This is the real world. There are mistakes all the time.

How many are we dealing with here?

```{r}
nrow(diamonds[diamonds$x == 0, ])
```
Let's kill it! That is let's *clean our data*!

```{r}
diamonds = diamonds[diamonds$x != 0, ]
```

How good does a best guess linear relationship do now?

```{r}
mod = lm(price ~ x, diamonds)
b = coef(mod)
b
summary(mod)$r.squared
summary(mod)$sigma
```

Let's see the best fit line $g(x)$ visually:

```{r}
ggplot(diamonds, aes(x = x, y = price)) + geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

Again we got some bad extrapolation going on which we can't fix using a purely linear modeling strategy.

Let's log-linearize it and see how we do.

```{r}
log_linear_mod = lm(log(price) ~ x, diamonds)
b = coef(log_linear_mod)
ggplot(diamonds, aes(x = x, y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

How did we do? Ensure it's apples-apples.

```{r}
log_y_hat = log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

Still not better. Log-log?

```{r}
log_log_linear_mod = lm(log(price) ~ log(x), diamonds)
b = coef(log_log_linear_mod)
ggplot(diamonds, aes(x = log(x), y = log(price))) + 
  geom_point() + 
  geom_abline(intercept = b[1], slope = b[2], col = "green")
```

How did we do? 

```{r}
log_y_hat = log_log_linear_mod$fitted.values
y_hat = exp(log_y_hat)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
```

We did it. We found a log transformation that seems to give higher predictive power than the vanilla linear model on the raw response and raw feature.

This brings up the whole idea of "model selection". We went hunting for models until we found one that's better.

Transforming y is a big decision as it changes the response metric! The rule of thumb is it is easier to model a response metric that has less extreme values (especially when using linear models) as the extreme values have a big impact on slope coefficients and can distort the best fit line due to the least squares minimization (hence the popularity of logging the response).

Let's see if we get anywhere with this using all the features in this model.

```{r}
lm_y = lm(price ~ ., diamonds)
lm_ln_y = lm(log(price) ~ ., diamonds)
summary(lm_y)$r.squared
summary(lm_y)$sigma

y_hat = exp(lm_ln_y$fitted.values)
e = diamonds$price - y_hat
SSE = sum(e^2)
SST = sum((diamonds$price - mean(diamonds$price))^2)
Rsq = 1 - sum(e^2) / SST
Rsq
RMSE = sqrt(SSE / (nrow(diamonds) - 2))
RMSE
``` 

This is pretty convincing evidence that this transformation does a better job (at least in our linear modeling context).

Let's look at one prediction:

```{r}
predict(lm_y, diamonds[12345, ])
exp(predict(lm_ln_y, diamonds[12345, ]))
diamonds$price[12345]
```

Again, we should be careful when you use $g$ after logging, you will have to exponentiate the result (middle line above). 

Small point: this exponentiation is known to create bias because $E[Y]$ is different from $exp(E[ln(y)])$ (for those who took 368 - remember Jensen's inequality?) For the purposes of this class, this can be ignored since we are evaluating g on its own merits and we're doing so honestly.

If you like this stuff, there are a whole bunch of transformations out there that are even cooler than the natural log. 


# Linear Models with Feature Interactions

Let's go back to modeling price with weight. Let us add a third variable to this plot, color, a metric about the "yellowness" of the diamond. This is an ordinal categorical variable ranging from D (most clear i.e. best) to J (most yellow in this dataset i.e. worst).


```{r}
pacman::p_load(ggplot2)
base = ggplot(diamonds, aes(x = carat, y = price)) 
base +
  geom_point(aes(col = color)) + 
  scale_color_brewer(type = "div")
```

We can split the data on color to see it more clearly:

```{r}
base +
  geom_point() +
  facet_wrap(~ color, ncol = 3) + 
  aes(color = color) + scale_color_brewer(type = "div")
```


What do we see here? It looks like the slope of the price vs. carat linear model is slightly affected by color. For instance, the "D" color diamonds' price increases much faster as weight increases than the "E" color diamonds' price increases in weight, etc. Why do you think this is?

We can investigate two of these linear models below by fitting two submodels, one for D and one for J:

```{r}
mod_D = lm(price ~ carat, diamonds[diamonds$color == "D", ])
b_D = coef(mod_D)
mod_J = lm(price ~ carat, diamonds[diamonds$color == "J", ])
b_J = coef(mod_J)
b_D
b_J
```

Let's see it on the plot:

```{r}
base +
  geom_point(aes(col = color)) + scale_color_brewer(type = "div") +
  geom_abline(intercept = b_D[1], slope = b_D[2], col = "blue", lwd = 2) +
  geom_abline(intercept = b_J[1], slope = b_J[2], col = "red", lwd = 2)
```

This indicates a separate intercept and carat-slope for each color. How is this done? Interacting carat and slope. The formula notation has the `*` operator for this. It is a type of abstract multiplication after all!

```{r}
mod = lm(price ~ color, diamonds)
coef(mod)
mod = lm(price ~ carat * color, diamonds)
coef(mod)
```

The reference category is color D. This means every other color should start lower and have a lower slope. This is approximately what we see above.

How much of a better model is this than a straight linear model?

```{r}
mod_vanilla = lm(price ~ carat + color, diamonds)
summary(mod_vanilla)$r.squared
summary(mod_vanilla)$sigma
summary(mod_vanilla)$df[1]
summary(mod)$r.squared
summary(mod)$sigma
summary(mod)$df[1]
```

You can get more predictive accuracy out of this. 

Why did degrees of freedom increase? 

Is this gain real? Yes. With six more df's and $n = 54,000$ there is no chance this gain came from overfitting noise. Add 10,000 garbage features, that's a different story.

Let's take a look at carat with another variable, depth, a continuous predictor. High depth indicates diamonds are skinny and tall; low depth indicates diamonds are flat like a pancake.

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(col = depth), lwd = 1, alpha = 0.5) + scale_colour_gradientn(colours = rainbow(5))
```

It seems people like flatter diamonds and are willing to pay more per carat. Let's see this in the regression:

```{r}
mod = lm(price ~ carat + depth, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * depth, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

Also a real performance boost.

How about cut?


```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(col = cut), lwd = 0.5) + scale_color_brewer(type = "div")
```

Looks like ideal diamonds increase more steeply with size vs lower cut diamonds. Let's see this in action:

```{r}
mod = lm(price ~ carat, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * cut, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

Yes.

Can we include all these interactions?

```{r}
mod = lm(price ~ carat + color + depth + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ carat * (color + depth + cut), diamonds)
summary(mod)$r.squared
summary(mod)$sigma
coef(mod)
```

A decent gain for all three.

What does the design matrix look like there? What is $p$?

```{r}
Xmm = model.matrix(price ~ carat * (color + depth + cut), diamonds)
head(Xmm)
```


Can we take a look at interactions of two categorical variables?


```{r}
plot1 = ggplot(diamonds, aes(x = cut, y = color)) +
  geom_jitter(aes(col = price), lwd = 0.5) + scale_colour_gradientn(colours = rainbow(5))
plot1
```
Let's see what the regressions say:


```{r}
mod = lm(price ~ color + cut, diamonds)
summary(mod)$r.squared
summary(mod)$sigma
mod = lm(price ~ color * cut, diamonds)
coef(mod)
summary(mod)$r.squared
summary(mod)$sigma
```

Not too much gain. Probably not worth it.


# More flexible H sets


```{r}
rm(list = ls())
pacman::p_load(ggplot2)
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
#drop some obviously nonsense observations
diamonds = diamonds[diamonds$carat <= 2 & diamonds$x != 0 & diamonds$y != 0 & diamonds$z != 0 & diamonds$depth != 0 & diamonds$table != 0,]
diamonds$ln_price = log(diamonds$price)
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
n = nrow(diamonds)
set.seed(1984)
diamonds = diamonds[sample(1 : n), ]
```

Note: I will now model price, not ln_price as ln_price yields residuals that are orders of magnitude. Fitting a good ln_price model will take more time.

```{r}
#All model formulas for reuse later
model_formulas = list(
  A = price ~ ln_carat,
  B = price ~ ln_carat * clarity,
  C = price ~ ln_carat * (clarity + cut + color),
  D = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut + color),
  E = price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + carat + x + y + z + depth + table) * (clarity + cut + color)
)
#Model A
mod = lm(model_formulas[["A"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model B
mod = lm(model_formulas[["B"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model C
mod = lm(model_formulas[["C"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model D
mod = lm(model_formulas[["D"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
#Model E 
mod = lm(model_formulas[["E"]], diamonds)
summary(mod)$sigma
mod$rank #i.e. degrees of freedom  = # vectors in colsp[X] to project onto
```

Big win on E... the reason I think is that now we have a flexible curve for each continuous feature and that curve changes with all the categorical features.

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!


```{r}
#Model F
model_formulas[["F"]] = price ~ 
        (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table + poly(carat, 3) + poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * (cut + color + clarity)
mod = lm(model_formulas[["F"]], diamonds)
summary(mod)$sigma
mod$rank
```

Even better (seemingly).

Can you think of any other way to expand the candidate set curlyH? Discuss.

We now have a very flexible curve for each continuous feature and that curve is fit individually for each level of all categories. We could indeed go further by allowing the levels among cut, color and clarity to interact and further allowing the continous features to interact. 

There is another strategy to increase the complexity of H without using function transforms of the features and interactions... we will have to see after winter break...

We should probably assess oos performance now. Sample 4,000 diamonds and use these to create a training set of 3,600 random diamonds and a test set of 400 random diamonds. Define K and do this splitting:

```{r}
K = 10
n_sub = 4000
set.seed(1984)
n_test = 1 / K * n_sub
n_train = n_sub - n_test
test_indicies = sample(1 : n, n_test)
train_indicies = sample(setdiff(1 : n, test_indicies), n_train)
all_other_indicies = setdiff(1 : n, c(test_indicies, train_indicies))
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 3600.

```{r}
oos_se = list()
all_models_train = list()
for (model_idx in LETTERS[1 : 6]){
  all_models_train[[model_idx]] = lm(model_formulas[[model_idx]], diamonds[train_indicies, ])
  summary(all_models_train[[model_idx]])$sigma
  oos_se[[model_idx]] = sd(diamonds$price[test_indicies] - predict(all_models_train[[model_idx]], diamonds[test_indicies, ]))
}
oos_se
```

You computed oos metrics only on n_* = 400 diamonds. What problem(s) do you expect in these oos metrics?

They are variable. And something is wrong with F! Possibly Runge's phenomenon?

Let's get a more stable estimate via K-fold cross validation:

```{r}
set.seed(1984)
temp = rnorm(n_sub)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 100)
```

Do the K-fold cross validation for all models and compute the overall s_e and s_s_e. 

```{r}
oos_se = list()
oos_s_se = list()
for (model_idx in LETTERS[1 : 6]){
  e_vec_k = list() #for each one
  for (k in 1 : K){
    test_indicies_k = which(folds_vec == k)
    train_indicies_k = which(folds_vec != k)
    mod = lm(model_formulas[[model_idx]], diamonds[train_indicies_k, ])
    e_vec_k[[k]] = sd(diamonds$price[test_indicies_k] - predict(mod, diamonds[test_indicies_k, ]))
  }
  oos_se[[model_idx]] = mean(unlist(e_vec_k)) #note: not exactly the overall sd, but close enough
  oos_s_se[[model_idx]] = sd(unlist(e_vec_k))
}
res = rbind(unlist(oos_se), unlist(oos_s_se))
rownames(res) = c("avg", "sd")
res
```

Model F seems more variable than all of them. Why? Possibly Runge's phenomenon?

Imagine using the entire rest of the dataset besides the 4,000 training observations divvied up into slices of 400. Measure the oos error for each slice and also plot it.

```{r}
n_step = 1 / K * n_sub
oos_se = list()
ses = list()
starting_ks = seq(from = 1, to = (length(all_other_indicies) - n_step), by = n_step)
for (model_idx in LETTERS[1 : 6]){
  se_k = list() #for each one
  for (k in 1 : length(starting_ks)){
    diamonds_k = diamonds[all_other_indicies[starting_ks[k] : (starting_ks[k] + n_step - 1)], ]
    se_k[[k]] = sd(diamonds_k$price - predict(all_models_train[[model_idx]], diamonds_k))
  }
  ses[[model_idx]] = se_k
  oos_se[[model_idx]] = unlist(se_k)
}

pacman::p_load(reshape2)
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model")
ggplot(reshape2::melt(oos_se)) + geom_boxplot(aes(x = L1, y = value)) + xlab("model") + ylim(0, 5000)
```

What do we learn from this? Model F is very risky. What exactly is going wrong? Part of the agony of data science is the "debugging" just like when debugging software. But here the errors can be in the modeling too! 

```{r}
max(unlist(ses[["F"]]))
k_BAD = which.max(unlist(ses[["F"]]))

diamonds_BAD = diamonds[all_other_indicies[starting_ks[k_BAD] : (starting_ks[k_BAD] + n_step - 1)], ]
diamonds_BAD

e_BAD = diamonds_BAD$price - predict(all_models_train[[model_idx]], diamonds_BAD)
tail(sort(abs(e_BAD)))
diamonds_BAD[which.max(abs(e_BAD)), ]

summary(diamonds[train_indicies, ])
```

Is it extrapolation? Yes... y = 58.9 and in a cubic function, that would be astronomical. The real mistake is that y = 58.9 is impossible. The data wasn't cleaned! 

**** Cleaning data essentially means visualizing each feature and the response (histograms for numeric and barplots for categorical) and ensuring there's no mistakes.

But this happens all the time and you don't want to be using polynomial functions for this reason since the new data may extrapolate very poorly.



## Model Selection and Three Data Splits

This unit is split into three use cases (1) Selection among M explicit models (2) Stepwise Model (3) Construction Hyperparameter Selection within one algorithm

# Use Case (II) Forward Stepwise Model Construction

There are many types of such stepwise models. Here we will look at Forward Stepwise Linear models. "Forward" meaning we start with a low complexity model and end with a high complexity model, "Stepwise" meaning we do so iteratively which each step consisting of one additional degree of freedom i.e. one incremental increase in complexity and "Linear" meaning that the model is linear. By default we use OLS.

We will be using the diamonds data again as an example. Let's make sure we have unordered factors to avoid issues later:

```{r}
rm(list = ls())
diamonds = ggplot2::diamonds
diamonds$cut = factor(diamonds$cut, ordered = FALSE)
diamonds$color = factor(diamonds$color, ordered = FALSE)
diamonds$clarity = factor(diamonds$clarity, ordered = FALSE)
```

What we're doing will be highly computational, so let's take a random sample of the diamonds in $\mathbb{D}$ for training, selecting and testing:

```{r}
Nsamp = 1300
set.seed(1984)
subindices = sample(1 : nrow(diamonds), Nsamp * 3)
diamonds_train = diamonds[subindices[1 : Nsamp], ]
diamonds_select = diamonds[subindices[(Nsamp + 1) : (2 * Nsamp)], ]
diamonds_test = diamonds[subindices[(2 * Nsamp + 1) : (3 * Nsamp)], ]
rm(subindices)
```

Let's built a model with all second-order interactions e.g. all things that look like depth x table x clarity or depth^2 x color or depth^3.

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
```

Here's what some of the feature names look like:

```{r}
sample(names(coef(mod)), 100)
```

For features that are non-binary, the total will be p_non_binary^3 features. Binary features are more complicated because its each level in feature A times each level in feature B. There are no squared or cube terms for binary features (since they're all the same i.e. ${0,1}^d = {0,1}$).

Remember we likely overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions.

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try predicting using this insano model on another Nsamp we didn't see...

```{r}
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ and awful RMSE --- why? We overfit big time!

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck! This means you can do ~60x better using the null model (average of y) instead of this model.

So let us employ stepwise to get a "good" model. We need our basis predictors to start with. How about the linear components of `. * . * .` --- there's nothing intrinsically wrong with that - it's probably a good basis for $f(x)$. Let's create the model matrix for both train and test:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)
Xmm_select = model.matrix(price ~ . * . * ., diamonds_select)
y_select = diamonds_select$price
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r}
included_features_by_iter = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (j_try %in% included_features_by_iter){
      next 
    }
    Xmm_sub = Xmm_train[, c(included_features_by_iter, j_try), drop = FALSE]
    all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
  }
  j_star = which.min(all_ses)
  included_features_by_iter = c(included_features_by_iter, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, included_features_by_iter, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_select = Xmm_select[, included_features_by_iter, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_select - y_hat_select)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i =", i, "in sample: se = ", round(all_ses[j_star], 1), "oos_se", round(oos_se, 1), "added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
}
```

Now let's look at our complexity curve:

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylim(0, max(c(simulation_results$in_sample_ses_by_iteration, simulation_results$oos_ses_by_iteration)))
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. There is a whole zone of flat models. Better to be simple. So let's pick it from the graph visually. You can also obviously do CV within each iterations to stabilize this further.

What's the optimal number of features? And its associated oos s_e?

```{r}
p_opt = 41 #change this based on plot above
oos_ses_by_iteration[p_opt]
```

What are those features?

```{r}
sort(colnames(Xmm_train)[included_features_by_iter[1 : p_opt]])
```

Lots of carat-color-clarity interactions which makes sense. Other features don't matter so much as their information is probably duplicated within carat, color and clarity.

What is the "true optimal model"? It's impossible to find as we'd have to search all subsets (of which there are exponential number of them 2^1000 = 10^31). Here we used a greedy search so hopefully we get somewhere in the ballpark of optimal. 

We now get a conservative estimate of its future performance:

```{r}
Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
optimal_mod = lm(diamonds_test$price ~ 0 + Xmm_test[, included_features_by_iter[1 : p_opt]])
summary(optimal_mod)$sigma
```

Looks even better than we found during the stepwise iterations on the select set. This is random variation. It would be better to use CV within the stepwise and CV outside to then stabilize this final estimate.

The final step is to fit g_final on all data using those selected features.




# The Grammar of graphics and ggplot

First load the package and the dataset of interest as a dataframe:

```{r}
pacman::p_load(ggplot2, quantreg)
cars = MASS::Cars93 #dataframe
```

ggplot is based on the "Grammar of Graphics", a concept invented by the Statistician / Computer Scientist Leland Wilkinson who worked on SPSS, Tableau and now he works at H20, software that analyzes big data. The reference of interest is [here](http://papers.rgrossman.com/proc-094.pdf). He drew on ideas from John Tukey (one of the great statistician of the previous generation) while he was at Bell Labs, Andreas Buja (one of my professors at Penn) and Jerome Friedman (the professor that taught my data mining course when I was in college at Stanford). 

It is a language that allows us to describe the components of a graphic. Previously, graphics were done in one shot and it was clunky. ggplot is a library written by Hadley Wickham based on this concept. Wickham is probably the most famous person in statistical computing today. He has commit rights in R and is one of the architects of RStudio. He calls grammar of graphics "graphical poems". Here are the basic components:

* an underlying data frame
* an "aesthetic" that maps visualization axes in the eventual plot(s) to variables in the data frame
* a "layer" which is composed of
  - a geometric object
  - a statistical transformation
  - a position adjustment
* a "scale" for each aesthetic
* a "coordinate" system for each aesthetic
* optional "facets" (more graphics)
* optional "labels" for the title, axes title, points, etc.

Don't worry - everything has "smart defaults" in Wickham's implementation so you don't have to worry about most things. We will explore some of the features below. Here's a good [cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

ggplot is layered where each component is an object. The objects are added to each other since the "+" operator is overloaded to accept these additions. This is nice because each component can be saved and reused. The following initialized the graphics:

```{r}
ggplot(cars)
```

Nothing happened - except the plot window went gray (the smart default). This is already rendering a graphic, but since it hasn't been given any of the required information, it has nothing to display. Next we create an aesthetics indicating a one-way plot (one variable only).

```{r}
ggplot(cars) + 
  aes(Price)
```

Notice how it can understand the variable name as an object name.

Since we've given it an aesthetics object, it now knows which variable is the x axis (default). It already knows the ranges of the variable (a smart default) and a default scale and coordinate system (smart defaults).

Usually this is done in one step by passing the aesthetics object into the ggplot:

```{r}
ggplot(cars, aes(Price))
```

Now we need to pick a layer by specifying a geometry. This is a type of plot. Since the predictor type of price is continuous, let's pick the "histogram" using the `geom_histogram` function:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram()
```

This can be customized:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram(binwidth = 1, col = "darkred", fill = "blue", alpha = 0.4)
```

Want to save it for your latex?

```{r}
ggsave("plot.png")
system("open plot.png")
ggsave("plot.pdf")
system("open plot.pdf")
```

Here are some other options besides the histogram:

```{r}
ggplot(cars, aes(Price)) +
  geom_dotplot()
ggplot(cars, aes(Price)) +
  geom_area(stat = "bin", binwidth = 2)
ggplot(cars, aes(Price)) +
  geom_freqpoly()
ggplot(cars, aes(Price)) +
  geom_density(fill = "green", alpha = 0.1)

summary(cars)
```


Can we compare price based on different conditions? Yes, we can subset the data and use color and alpha:

```{r}
ggplot(cars, aes(Price)) +
  geom_density(data = subset(cars, Man.trans.avail == "Yes"), col = "grey", fill = "darkgreen", alpha = 0.4) +
  geom_density(data = subset(cars, Man.trans.avail == "No"), col = "grey", fill = "red", alpha = 0.4)
```

Sidebar: why are cars that have manual transmissions available cheaper?

We can look at this also using a histogram of the conditional distributions:

```{r}
ggplot(cars, aes(Price)) +
  geom_histogram(data = subset(cars, Man.trans.avail == "Yes"), binwidth = 1, col = "grey", fill = "darkgreen", alpha = 0.4) +
  geom_histogram(data = subset(cars, Man.trans.avail == "No"), binwidth = 1, col = "grey", fill = "red", alpha = 0.4)
```

What if the variable is not continuous e.g. Cylinders? We can use a bar graph / bar plot.

```{r}
ggplot(cars, aes(Cylinders)) +
  geom_bar()
```

This is essential frequency by level of the categorical variable.

Now let's move on to looking at one variable versus another variable. For example price by engine power:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price))
```

Since we've given it an aesthetics object, it now knows which variable is the x axis and which variable is the y axis. It already knows the ranges of the variables (a smart default) and a default scale and coordinate system (smart defaults).

Just as before, now we need to pick a layer by specifying a geometry. This is a type of plot. Let's pick the "scatterplot" using the `geom_point` function:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point()
```

Now we have a nice scatterplot. This function uses the inherited data, the inherited aesthetics. Since this "geometry" is a "layer", we can pass in options to the layer.

```{r}
base_and_aesthetics = ggplot(cars, aes(x = Horsepower, y = Price))
base_and_aesthetics + 
  geom_point(col = "red", fill = "green", shape = 23, size = 3, alpha = 0.3)
```

Let's handle names of axes, title and ranges:

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics +
  ggtitle("Average Car Price vs. Engine Power", subtitle = "in the Cars93 dataset") +
  ylab("Price (in $1000's)")
base_and_aesthetics_with_titles +
  geom_point() +
  xlim(0, 400) +
  ylim(0, 50)
  
```

Let's transform the variables:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_x_continuous(trans = "log2")
```

Each unit increase on the x-axis now represent a doubling increase in x (although the whole scale only spans 3 units). But look at how the grid didn't keep up. Let's fix this:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_x_continuous(trans = "log2", breaks = round(seq(0, max(cars$Horsepower), length.out = 6)))
```

We can do the same to the y axis:

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  scale_y_continuous(trans = "log10")+
  scale_x_continuous(trans = "log10")
```

Let's look at some more geometries.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth()
```

Here, I've added two geometries on the same aesthetic! This attempts to explain the relationship $f(x)$ using smoothing. Let's go for more.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_rug()
```

This allows us to also see the marginal distributions of the two variables.

```{r}
base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_quantile(col = "red") +
  geom_rug()
```

This fits a line and tries to indicate statistical significance of the line. We have *not* covered any statistics in this class yet (ironic!) ... so ignore how the window is generated.

Can we display more than two dimensions? Yes. Let's indicate a third dimension with shape (only works with factors).

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power and Transmission")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail)) +
  geom_smooth() +
  geom_rug()
```

Can we display more than three dimensions? Yes. Let's indicate a fourth dimension with color.

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission & Drivetrain")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain)) +
  geom_smooth() +
  geom_rug()
```

Can we go to a fifth dimension? Maybe?

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission & Drivetrain")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain, size = Weight), alpha = 0.5) + #size?
  geom_smooth() +
  geom_rug()
```

A seventh? We can use text labels adjacent to the scatterplot's points.

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power, Transmission, Drivetrain,  Weight & #Cylinders")
base_and_aesthetics_with_titles +
  geom_point(aes(shape = Man.trans.avail, col = DriveTrain, alpha = Weight)) + #size?
  geom_text(aes(label = Cylinders), vjust = 1.5, col = "darkgrey", lineheight = 0.3, size = 3) +
  geom_smooth() +
  geom_rug()
```

Getting difficult to see what's going on.

Let's move away from the scatterplot to just density estimation:

```{r}
base_and_aesthetics_with_titles = base_and_aesthetics_with_titles +
  ggtitle("Average Car Price by Power") #reset the title
base_and_aesthetics_with_titles +
  geom_density2d()
```

Other alternatives:

```{r}
base_and_aesthetics_with_titles +
  geom_bin2d(binwidth = c(8, 3))
pacman::p_load(hexbin)
base_and_aesthetics_with_titles +
  geom_hex()
```

This is like a two-dimensional histogram where the bar / hexagon heights are seen with color.

What if the x-axis is categorical for example Cylinders versus price? Typical is the "box and whiskers" plot:

```{r}
ggplot(cars, aes(x = Cylinders, y = Price)) +
  geom_boxplot()
```

Clear relationship!

How about multiple subplots based on the subsetting we did in the histograms? This is called "faceting". Here are two bivariate visualizations laid horizontally:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  geom_smooth() +
  facet_grid(. ~ Man.trans.avail)
```

Or alternatively, vertically:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  geom_smooth() +
  facet_grid(Man.trans.avail ~ .)
```

And we can even double-subset:

```{r}
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  facet_grid(Man.trans.avail ~ Origin)
```

And we can even triple-subset or more:

```{r}
cars$MedWeight = ifelse(cars$Weight > median(cars$Weight), ">MedWeight", "<MedWeight")
ggplot(cars, aes(x = Horsepower, y = Price)) +
  geom_point() +
  facet_grid(Man.trans.avail ~ Origin + MedWeight, scales = "free")
```

These three varibles seem somewhat independent.

There are other primitives like `geom_abline` which graphs a line and `geom_segment` we will see today. Note that if you want plots rendered within functions or loops you have to explicitly call the `plot` function:

```{r}
for (nsim in 1 : 3){
  graphics_obj = ggplot(data.frame(x = rnorm(1000))) + 
    geom_histogram(aes(x))
  graphics_obj
}
```

versus:

```{r}
for (nsim in 1 : 3){
  graphics_obj = ggplot(data.frame(x = rnorm(1000))) + geom_histogram(aes(x))
  plot(graphics_obj)
}
```


Lastly, ggplot offers lots of nice customization themes:

```{r}
graphics_obj = base_and_aesthetics_with_titles +
  geom_point() +
  geom_smooth() +
  geom_quantile(col = "red") +
  geom_rug()
graphics_obj + theme_bw()
graphics_obj + theme_dark()
graphics_obj + theme_classic()

```

Packages offer even more:

```{r}
pacman::p_load(forcats, lazyeval, ggthemes)
graphics_obj + theme_economist()
graphics_obj + theme_stata()
graphics_obj + theme_tufte()
```

and of course, the whimsical one and only:


```{r}
pacman::p_load(xkcd, extrafont)
download.file("http://simonsoftware.se/other/xkcd.ttf", dest = "xkcd.ttf", mode = "wb")
#MAC
# system("mv xkcd.ttf /Library/Fonts")
# font_import(path = "/Library/Fonts", pattern = "xkcd", prompt = FALSE)
# fonts()
# fonttable()
# loadfonts()
#WINDOWS
font_import(path = ".", pattern = "xkcd", prompt = FALSE)
fonts()
fonttable()

loadfonts(device="win")

graphics_obj + theme_xkcd()
```

